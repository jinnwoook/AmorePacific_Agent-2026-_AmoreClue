"""
LLM Inference Server for AMORE CLUE Dashboard
- Text: EXAONE-3.5-7.8B-Instruct (LG AI Research)
- Multimodal: Qwen2-VL-2B-Instruct (Lazy Loading)
Endpoints: rag-insight, plc-prediction, category-prediction, chat
"""
import os
import json
import re
import base64
import io
import torch
import setproctitle
import threading
import time
import gc
setproctitle.setproctitle("wook-llm-port7")
from flask import Flask, request, jsonify
from transformers import AutoModelForCausalLM, AutoTokenizer, Qwen2VLForConditionalGeneration, AutoProcessor
from PIL import Image

app = Flask(__name__)

# ===== CUDA Error Handling =====
# ë™ì‹œ ìš”ì²­ ì œí•œ (GPU ì¶©ëŒ ë°©ì§€)
inference_semaphore = threading.Semaphore(1)
MAX_RETRIES = 2
CUDA_ERROR_COUNT = 0
MAX_CUDA_ERRORS = 5  # ì´ íšŸìˆ˜ ì´ˆê³¼ì‹œ ì„œë²„ ì¬ì‹œì‘ ê¶Œì¥

def reset_cuda_state():
    """CUDA ìƒíƒœ ì´ˆê¸°í™”"""
    global CUDA_ERROR_COUNT
    try:
        torch.cuda.empty_cache()
        gc.collect()
        print("[CUDA] Memory cache cleared")
    except Exception as e:
        print(f"[CUDA] Cache clear failed: {e}")
    CUDA_ERROR_COUNT += 1
    if CUDA_ERROR_COUNT >= MAX_CUDA_ERRORS:
        print(f"[WARNING] CUDA errors exceeded {MAX_CUDA_ERRORS}. Server restart recommended.")

# ===== VLM Model (Lazy Loading) =====
vlm_model = None
vlm_processor = None
VLM_MODEL_NAME = "Qwen/Qwen2-VL-2B-Instruct"

# GPU ì„¤ì •
DEVICE = "cuda:7"
MODEL_NAME = "LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct"

print(f"Loading model: {MODEL_NAME} on {DEVICE}...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.bfloat16,  # float16 â†’ bfloat16 (ìˆ˜ì¹˜ ì•ˆì •ì„± ê°œì„ )
    device_map=DEVICE,
    trust_remote_code=True,
)
model.eval()
print("Model loaded successfully!")


def load_vlm_model():
    """Lazy load Qwen2-VL model when first multimodal request comes in"""
    global vlm_model, vlm_processor
    if vlm_model is None:
        print(f"Loading VLM model: {VLM_MODEL_NAME} on {DEVICE}...")
        from qwen_vl_utils import process_vision_info
        vlm_processor = AutoProcessor.from_pretrained(VLM_MODEL_NAME, trust_remote_code=True)
        vlm_model = Qwen2VLForConditionalGeneration.from_pretrained(
            VLM_MODEL_NAME,
            torch_dtype=torch.float16,
            device_map=DEVICE,
            trust_remote_code=True,
        )
        vlm_model.eval()
        print(f"VLM model loaded successfully on {DEVICE}!")
    return vlm_model, vlm_processor


# ===== RAG: ìƒˆ ì„ë² ë”© ë°ì´í„° (ë§ˆì¼€íŒ… ì‚¬ë¡€ + ì‹œì¥ ì‹ í˜¸) =====
import numpy as np
from transformers import AutoTokenizer as EmbedTokenizer, AutoModel as EmbedModel

RAG_DATA_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'rag_data', 'rag_embeddings.json')
print(f"Loading RAG embeddings from: {RAG_DATA_PATH}")

RAG_AVAILABLE = False
rag_data = None
rag_embeddings = None
embed_tokenizer = None
embed_model_rag = None

def mean_pooling_rag(model_output, attention_mask):
    """Mean pooling for sentence embeddings"""
    token_embeddings = model_output[0]
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)

def get_query_embedding(query_text):
    """ì¿¼ë¦¬ í…ìŠ¤íŠ¸ì˜ ì„ë² ë”© ìƒì„±"""
    global embed_tokenizer, embed_model_rag

    if embed_tokenizer is None:
        return None

    encoded = embed_tokenizer([query_text], padding=True, truncation=True, max_length=512, return_tensors='pt')

    with torch.no_grad():
        output = embed_model_rag(**encoded)

    embedding = mean_pooling_rag(output, encoded['attention_mask'])
    embedding = torch.nn.functional.normalize(embedding, p=2, dim=1)
    return embedding.numpy()[0]

def search_rag(query_text, insight_type="marketing", top_k=3):
    """RAG ê²€ìƒ‰ - ìœ ì‚¬í•œ ë¬¸ì„œ ë°˜í™˜"""
    global rag_data, rag_embeddings

    if not RAG_AVAILABLE or rag_data is None:
        return []

    query_embedding = get_query_embedding(query_text)
    if query_embedding is None:
        return []

    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
    similarities = np.dot(rag_embeddings, query_embedding)

    # insight_typeì— ë”°ë¼ í•„í„°ë§ (marketingì€ marketing_case, npd/overseasëŠ” ë‘˜ ë‹¤)
    filtered_indices = []
    for i, doc in enumerate(rag_data['documents']):
        if insight_type == "marketing" and doc['type'] == 'marketing_case':
            filtered_indices.append(i)
        elif insight_type in ["npd", "overseas"]:
            # NPDì™€ í•´ì™¸ì§„ì¶œì€ ë‘˜ ë‹¤ ì°¸ê³ 
            filtered_indices.append(i)

    # í•„í„°ë§ëœ ë¬¸ì„œì—ì„œ ìƒìœ„ Kê°œ ì„ íƒ
    if not filtered_indices:
        filtered_indices = list(range(len(rag_data['documents'])))

    filtered_sims = [(i, similarities[i]) for i in filtered_indices]
    filtered_sims.sort(key=lambda x: x[1], reverse=True)
    top_results = filtered_sims[:top_k]

    results = []
    for idx, sim in top_results:
        doc = rag_data['documents'][idx]
        results.append({
            'id': doc['id'],
            'type': doc['type'],
            'country': doc.get('country', ''),
            'brand': doc.get('brand', ''),
            'product': doc.get('product', ''),
            'category': doc.get('category', ''),
            'text': doc.get('text', ''),
            'similarity': float(sim),
            # ë§ˆì¼€íŒ… ì‚¬ë¡€ ì¶”ê°€ ì •ë³´
            'why_it_worked': doc.get('why_it_worked', ''),
            'evidence_snippet': doc.get('evidence_snippet', ''),
            'key_message': doc.get('key_message', ''),
            'channel': doc.get('channel', ''),
            # ì‹œì¥ ì‹ í˜¸ ì¶”ê°€ ì •ë³´
            'signal_type': doc.get('signal_type', ''),
            'signal_strength': doc.get('signal_strength', ''),
            'evidence_summary': doc.get('evidence_summary', ''),
        })

    return results

try:
    if os.path.exists(RAG_DATA_PATH):
        with open(RAG_DATA_PATH, 'r', encoding='utf-8') as f:
            rag_data = json.load(f)

        # ì„ë² ë”© ë°°ì—´ ì¶”ì¶œ
        rag_embeddings = np.array([doc['embedding'] for doc in rag_data['documents']])

        # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (ì¿¼ë¦¬ìš©)
        EMBED_MODEL_NAME = rag_data.get('model', 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
        embed_tokenizer = EmbedTokenizer.from_pretrained(EMBED_MODEL_NAME)
        embed_model_rag = EmbedModel.from_pretrained(EMBED_MODEL_NAME)
        embed_model_rag.eval()

        RAG_AVAILABLE = True
        print(f"RAG embeddings loaded: {rag_data['total_documents']} documents, {rag_data['dimension']}D")
    else:
        print(f"WARNING: RAG embeddings file not found: {RAG_DATA_PATH}")
except Exception as e:
    print(f"WARNING: RAG not available: {e}")
    RAG_AVAILABLE = False

# ë ˆê±°ì‹œ ë³€ìˆ˜ (í˜¸í™˜ì„±)
embed_model = None
rag_collections = {}


SYSTEM_PROMPT = """ë‹¹ì‹ ì€ ê¸€ë¡œë²Œ K-ë·°í‹°(K-Beauty) ì‹œì¥ ë¶„ì„ ë° í™”ì¥í’ˆ ì‚°ì—… íŠ¸ë Œë“œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì•„ëª¨ë ˆí¼ì‹œí”½, LGìƒí™œê±´ê°• ë“± í•œêµ­ í™”ì¥í’ˆ ê¸°ì—…ì˜ ê¸€ë¡œë²Œ ì „ëµì„ ìë¬¸í•˜ëŠ” ìˆ˜ì¤€ì˜ ì „ë¬¸ì„±ì„ ê°–ì¶”ê³  ìˆìŠµë‹ˆë‹¤.
ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹¬ì¸µì ì´ê³  ì „ë¬¸ì ì¸ ë¶„ì„ì„ ì œê³µí•˜ë˜, ë°ì´í„°ì— ë‚˜íƒ€ë‚˜ì§€ ì•ŠëŠ” ì‹œì¥ ë§¥ë½, ì†Œë¹„ì ì‹¬ë¦¬, ì‚°ì—… ë™í–¥ê¹Œì§€ ì¢…í•©ì ìœ¼ë¡œ ê³ ë ¤í•˜ì—¬ í’ë¶€í•œ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”."""


def generate_response(prompt: str, max_new_tokens: int = 1024) -> str:
    """Generate a response from the LLM with CUDA error handling"""
    global CUDA_ERROR_COUNT

    # ë™ì‹œ ìš”ì²­ ì œí•œ (ì„¸ë§ˆí¬ì–´ë¡œ 1ê°œì”© ì²˜ë¦¬)
    acquired = inference_semaphore.acquire(timeout=120)  # ìµœëŒ€ 2ë¶„ ëŒ€ê¸°
    if not acquired:
        raise RuntimeError("Inference timeout: too many concurrent requests")

    try:
        for attempt in range(MAX_RETRIES + 1):
            try:
                messages = [
                    {"role": "system", "content": SYSTEM_PROMPT},
                    {"role": "user", "content": prompt}
                ]

                text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
                inputs = tokenizer(text, return_tensors="pt").to(DEVICE)

                with torch.no_grad():
                    outputs = model.generate(
                        **inputs,
                        max_new_tokens=max_new_tokens,
                        temperature=0.75,  # 0.7 â†’ 0.75 (ìˆ˜ì¹˜ ì•ˆì •ì„±)
                        top_p=0.9,
                        top_k=50,  # ì¶”ê°€: ìƒ˜í”Œë§ í’€ ì œí•œ
                        do_sample=True,
                        repetition_penalty=1.05,  # 1.1 â†’ 1.05 (inf/nan ë°©ì§€)
                    )

                generated = outputs[0][inputs["input_ids"].shape[1]:]
                response = tokenizer.decode(generated, skip_special_tokens=True)

                # ì„±ê³µ ì‹œ ì—ëŸ¬ ì¹´ìš´íŠ¸ ë¦¬ì…‹
                CUDA_ERROR_COUNT = max(0, CUDA_ERROR_COUNT - 1)
                return response.strip()

            except RuntimeError as e:
                error_msg = str(e)
                is_cuda_error = "CUDA" in error_msg or "device-side assert" in error_msg or "out of memory" in error_msg.lower()
                is_prob_error = "probability tensor" in error_msg or "inf" in error_msg or "nan" in error_msg

                if is_cuda_error or is_prob_error:
                    print(f"[INFERENCE ERROR] Attempt {attempt + 1}/{MAX_RETRIES + 1}: {error_msg[:100]}")
                    reset_cuda_state()

                    if attempt < MAX_RETRIES:
                        time.sleep(2)  # ì ì‹œ ëŒ€ê¸° í›„ ì¬ì‹œë„
                        continue
                    else:
                        raise RuntimeError(f"Inference error after {MAX_RETRIES + 1} attempts. Server restart may be needed.")
                else:
                    raise  # ê¸°íƒ€ ì—ëŸ¬ëŠ” ê·¸ëŒ€ë¡œ ì „íŒŒ

    finally:
        # í•­ìƒ ì„¸ë§ˆí¬ì–´ í•´ì œ ë° ë©”ëª¨ë¦¬ ì •ë¦¬
        inference_semaphore.release()
        torch.cuda.empty_cache()


def clean_text(text: str) -> str:
    """Remove markdown formatting from LLM output"""
    import re
    # Remove bold/italic markers
    text = text.replace("**", "").replace("*", "")
    # Remove heading markers (##, ###, etc.) at start of lines
    text = re.sub(r'^#{1,6}\s*', '', text, flags=re.MULTILINE)
    # Remove links [text](url) -> text
    text = re.sub(r'\[([^\]]+)\]\([^\)]+\)', r'\1', text)
    # Remove code markers
    text = text.replace("``", "").replace("`", "")
    # Remove any remaining markdown-style formatting
    text = re.sub(r'^\s*[-*+]\s+', 'â€¢ ', text, flags=re.MULTILINE)  # Convert list markers to bullet
    return text.strip()


@app.route("/api/llm/review-summary", methods=["POST"])
def review_summary():
    """ë¦¬ë·° AI ë¶„ì„ ìš”ì•½ ìƒì„± - ì¢…í•©ì ì´ê³  ìƒì„¸í•œ ë¶„ì„"""
    try:
        data = request.json
        keyword = data.get("keyword", "")
        country = data.get("country", "usa")
        positive_keywords = data.get("positiveKeywords", [])
        negative_keywords = data.get("negativeKeywords", [])
        positive_count = data.get("positiveCount", 0)
        negative_count = data.get("negativeCount", 0)
        is_combination = data.get("isCombination", False)

        country_names = {
            "usa": "ë¯¸êµ­", "japan": "ì¼ë³¸", "singapore": "ì‹±ê°€í¬ë¥´",
            "malaysia": "ë§ë ˆì´ì‹œì•„", "indonesia": "ì¸ë„ë„¤ì‹œì•„"
        }
        country_name = country_names.get(country, "í•´ì™¸")

        pos_list = ", ".join([f"{k['keyword']}({k['count']}ê±´)" for k in positive_keywords[:8]])
        neg_list = ", ".join([f"{k['keyword']}({k['count']}ê±´)" for k in negative_keywords[:8]])

        total = positive_count + negative_count
        pos_ratio = round(positive_count / total * 100, 1) if total > 0 else 50

        item_type = "ê¿€ì¡°í•©(ì„±ë¶„+ì œí˜•+íš¨ê³¼ ì¡°í•©) í‚¤ì›Œë“œ" if is_combination else "íŠ¸ë Œë“œ í‚¤ì›Œë“œ"

        prompt = f"""ë‹¤ìŒì€ {country_name} ì‹œì¥ì—ì„œ "{keyword}" {item_type}ì— ëŒ€í•œ ì†Œë¹„ì ë¦¬ë·° ë¶„ì„ ë°ì´í„°ì…ë‹ˆë‹¤.

[ë¦¬ë·° ë°ì´í„° í˜„í™©]
- ì „ì²´ ë¦¬ë·° ìˆ˜: {total}ê±´
- ê¸ì • ë¦¬ë·°: {positive_count}ê±´ ({pos_ratio}%)
- ë¶€ì • ë¦¬ë·°: {negative_count}ê±´ ({round(100 - pos_ratio, 1)}%)
- ê¸ì • ë¦¬ë·° ì£¼ìš” í‚¤ì›Œë“œ: {pos_list}
- ë¶€ì • ë¦¬ë·° ì£¼ìš” í‚¤ì›Œë“œ: {neg_list}

ìœ„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°€ë…ì„± ì¢‹ì€ í˜•ì‹ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”. ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ì„ ì •í™•íˆ ë”°ë¼ì£¼ì„¸ìš”:

[ì†Œë¹„ìë°˜ì‘]
â€¢ ê¸ì • ìš”ì¸: ê¸ì • í‚¤ì›Œë“œ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì†Œë¹„ìë“¤ì´ ë§Œì¡±í•˜ëŠ” í•µì‹¬ í¬ì¸íŠ¸ 2-3ê°€ì§€ë¥¼ êµ¬ì²´ì  ìˆ˜ì¹˜ì™€ í•¨ê»˜ ì„¤ëª…
â€¢ ë¶€ì • ìš”ì¸: ë¶€ì • í‚¤ì›Œë“œ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì†Œë¹„ìë“¤ì´ ë¶ˆë§Œì¡±í•˜ëŠ” í¬ì¸íŠ¸ 1-2ê°€ì§€ë¥¼ êµ¬ì²´ì  ìˆ˜ì¹˜ì™€ í•¨ê»˜ ì„¤ëª…

[í•µì‹¬ì¸ì‚¬ì´íŠ¸]
1. ì²« ë²ˆì§¸ ì¸ì‚¬ì´íŠ¸ (ë°ì´í„° ê·¼ê±° í¬í•¨)
2. ë‘ ë²ˆì§¸ ì¸ì‚¬ì´íŠ¸ (ë°ì´í„° ê·¼ê±° í¬í•¨)
3. ì„¸ ë²ˆì§¸ ì¸ì‚¬ì´íŠ¸ (ë°ì´í„° ê·¼ê±° í¬í•¨)

[ì‹œì¥ì „ë§]
{country_name} ì‹œì¥ì—ì„œ "{keyword}" í‚¤ì›Œë“œì˜ í–¥í›„ ì „ë§ê³¼ K-ë·°í‹° ë¸Œëœë“œ ê´€ì ì—ì„œì˜ ê¸°íšŒ/ì£¼ì˜ì ì„ 2-3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½

[í‚¤ì›Œë“œ] í•µì‹¬ ì¸ì‚¬ì´íŠ¸ í‚¤ì›Œë“œ 4ê°œë¥¼ ì‰¼í‘œë¡œ êµ¬ë¶„ (ê° 2-6ì)"""

        response = generate_response(prompt, max_new_tokens=1500)
        # ë§ˆí¬ë‹¤ìš´ í¬ë§·íŒ… ì œê±°
        response = clean_text(response)

        # Parse response - ìƒˆë¡œìš´ êµ¬ì¡°í™”ëœ í˜•ì‹
        import re
        consumer_response = ""
        insight_list = []
        market_outlook = ""
        keywords = []
        sentiment_ratio = pos_ratio / 100

        lines = response.split("\n")
        current_section = None
        consumer_lines = []
        insight_lines = []
        outlook_lines = []

        for line in lines:
            line = line.strip()
            if not line:
                continue

            # ì„¹ì…˜ í—¤ë” ê°ì§€
            if any(kw in line for kw in ["[ì†Œë¹„ìë°˜ì‘]", "ì†Œë¹„ìë°˜ì‘:", "ì†Œë¹„ì ë°˜ì‘"]):
                current_section = "consumer"
                continue
            elif any(kw in line for kw in ["[í•µì‹¬ì¸ì‚¬ì´íŠ¸]", "í•µì‹¬ì¸ì‚¬ì´íŠ¸:", "í•µì‹¬ ì¸ì‚¬ì´íŠ¸"]):
                current_section = "insights"
                continue
            elif any(kw in line for kw in ["[ì‹œì¥ì „ë§]", "ì‹œì¥ì „ë§:", "ì‹œì¥ ì „ë§"]):
                current_section = "outlook"
                continue
            elif any(kw in line for kw in ["[í‚¤ì›Œë“œ]", "í‚¤ì›Œë“œ:"]):
                rest = re.sub(r'^\[í‚¤ì›Œë“œ\]|^í‚¤ì›Œë“œ:', '', line).strip()
                if rest:
                    keywords = [k.strip() for k in rest.split(",") if k.strip()][:5]
                current_section = None
                continue

            # ì„¹ì…˜ë³„ ë‚´ìš© ìˆ˜ì§‘
            if current_section == "consumer":
                consumer_lines.append(line)
            elif current_section == "insights":
                clean_line = line.lstrip("0123456789.-â€¢â†’Â·)#* ").strip()
                if clean_line and len(clean_line) > 5:
                    insight_lines.append(clean_line)
            elif current_section == "outlook":
                outlook_lines.append(line)

        # ê²°ê³¼ ì¡°í•©
        consumer_response = "\n".join(consumer_lines) if consumer_lines else ""
        insight_list = insight_lines[:4] if insight_lines else []
        market_outlook = " ".join(outlook_lines) if outlook_lines else ""

        # ì „ì²´ ìš”ì•½ ìƒì„± (êµ¬ì¡°í™”ëœ í˜•ì‹)
        summary_parts = []
        if consumer_response:
            summary_parts.append(f"ğŸ“Š ì†Œë¹„ì ë°˜ì‘\n{consumer_response}")
        if insight_list:
            insights_text = "\n".join([f"{i+1}. {ins}" for i, ins in enumerate(insight_list)])
            summary_parts.append(f"ğŸ” í•µì‹¬ ì¸ì‚¬ì´íŠ¸\n{insights_text}")
        if market_outlook:
            summary_parts.append(f"ğŸ’¡ ì‹œì¥ ì „ë§\n{clean_text(market_outlook)}")

        summary = "\n\n".join(summary_parts) if summary_parts else ""

        # Fallback
        if not summary:
            summary = f"ğŸ“Š ì†Œë¹„ì ë°˜ì‘\nâ€¢ ê¸ì • ë¹„ìœ¨ {pos_ratio}%ë¡œ ì „ë°˜ì ìœ¼ë¡œ í˜¸ì˜ì ì¸ ë°˜ì‘\nâ€¢ ì£¼ìš” ê¸ì • í‚¤ì›Œë“œ: {pos_list[:100]}\n\nğŸ” í•µì‹¬ ì¸ì‚¬ì´íŠ¸\n1. {keyword}ì— ëŒ€í•œ ì†Œë¹„ì ê´€ì‹¬ë„ ìƒìŠ¹\n2. ë¦¬ë·° ë°ì´í„° ê¸°ë°˜ ì‹œì¥ ì ì¬ë ¥ í™•ì¸\n\nğŸ’¡ ì‹œì¥ ì „ë§\n{country_name} ì‹œì¥ì—ì„œ {keyword} í‚¤ì›Œë“œì˜ ì„±ì¥ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤."
        if not keywords:
            keywords = ["íš¨ê³¼ ìš°ìˆ˜", "ë³´ìŠµë ¥", "ê°€ì„±ë¹„", "ë§Œì¡±ë„"]

        return jsonify({
            "success": True,
            "summary": summary,
            "insights": keywords,
            "sentimentRatio": sentiment_ratio,
        })

    except Exception as e:
        print(f"Error in review_summary: {e}")
        return jsonify({"success": False, "error": str(e)}), 500


@app.route("/api/llm/category-trend", methods=["POST"])
def category_trend():
    """ì¹´í…Œê³ ë¦¬ ì „ì²´ í‚¤ì›Œë“œ ê²½í–¥ì„± ê¸°ë°˜ íŠ¸ë Œë“œ ë¶„ì„"""
    try:
        data = request.json
        country = data.get("country", "usa")
        category = data.get("category", "Skincare")
        top_keywords = data.get("topKeywords", [])

        country_names = {
            "usa": "ë¯¸êµ­", "japan": "ì¼ë³¸", "singapore": "ì‹±ê°€í¬ë¥´",
            "malaysia": "ë§ë ˆì´ì‹œì•„", "indonesia": "ì¸ë„ë„¤ì‹œì•„"
        }
        country_name = country_names.get(country, "í•´ì™¸")

        keywords_text = ""
        if top_keywords:
            keywords_text = "\n".join([f"  - {k.get('keyword', '')} (ì ìˆ˜: {k.get('score', 0)}, íŠ¸ë Œë“œ: {k.get('trendLevel', '')})" for k in top_keywords[:15]])

        prompt = f"""ë‹¤ìŒì€ {country_name} {category} ì¹´í…Œê³ ë¦¬ì˜ ì „ì²´ í‚¤ì›Œë“œ ê²½í–¥ì„± ë°ì´í„°ì…ë‹ˆë‹¤.

[ì¹´í…Œê³ ë¦¬ ë°ì´í„°]
- êµ­ê°€: {country_name}
- ì¹´í…Œê³ ë¦¬: {category}
- ìƒìœ„ í‚¤ì›Œë“œ ëª©ë¡:
{keywords_text}

ìœ„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ, {country_name} {category} ì¹´í…Œê³ ë¦¬ì˜ ì „ì²´ì ì¸ íŠ¸ë Œë“œ ê²½í–¥ì„±ì„ ì¢…í•© ë¶„ì„í•´ì£¼ì„¸ìš”.

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì •í™•íˆ ë‹µë³€í•´ì£¼ì„¸ìš”:

[ì„¤ëª…]
ì´ ì¹´í…Œê³ ë¦¬ì˜ ì „ë°˜ì ì¸ íŠ¸ë Œë“œ ë°©í–¥ì„±ì„ 5-7ë¬¸ì¥ìœ¼ë¡œ ìƒì„¸í•˜ê²Œ ë¶„ì„í•´ì£¼ì„¸ìš”. ì£¼ìš” í‚¤ì›Œë“œ ê°„ì˜ ê´€ê³„, ì†Œë¹„ì ë‹ˆì¦ˆ ë³€í™”, ì‹œì¥ íë¦„ì„ í¬í•¨í•´ì£¼ì„¸ìš”.

[í•µì‹¬ìš”ì¸]
ì´ ì¹´í…Œê³ ë¦¬ íŠ¸ë Œë“œì˜ í•µì‹¬ ë™ì¸ 4-5ê°œë¥¼ ê°ê° í•œ ì¤„ì”© ì‘ì„±í•´ì£¼ì„¸ìš”."""

        response = generate_response(prompt, max_new_tokens=800)

        import re as re2
        explanation = ""
        key_factors = []
        current_section = None
        explanation_lines = []

        lines = response.split("\n")
        for line in lines:
            line = line.strip()
            if not line:
                continue
            if any(kw in line for kw in ["[ì„¤ëª…]", "ì„¤ëª…:", "## ì„¤ëª…"]):
                current_section = "explanation"
                rest = re2.sub(r'^\[ì„¤ëª…\]|^ì„¤ëª…:|^## ì„¤ëª…', '', line).strip()
                if rest and len(rest) > 5:
                    explanation_lines.append(rest)
            elif any(kw in line for kw in ["[í•µì‹¬ìš”ì¸]", "í•µì‹¬ìš”ì¸:", "## í•µì‹¬ìš”ì¸", "í•µì‹¬ ìš”ì¸", "[í•µì‹¬ ìš”ì¸]"]):
                current_section = "factors"
                rest = re2.sub(r'^\[í•µì‹¬\s*ìš”ì¸\]|^í•µì‹¬\s*ìš”ì¸:|^## í•µì‹¬\s*ìš”ì¸', '', line).strip()
                if rest and len(rest) > 3:
                    key_factors.append(clean_text(rest))
            elif current_section == "explanation":
                clean_line = line.lstrip("0123456789.-â€¢â†’Â·)#* ").strip()
                if clean_line and len(clean_line) > 5:
                    explanation_lines.append(clean_line)
            elif current_section == "factors":
                clean_line = line.lstrip("0123456789.-â€¢â†’Â·)#* ").strip()
                if clean_line and len(clean_line) > 3:
                    key_factors.append(clean_text(clean_line))

        explanation = clean_text(" ".join(explanation_lines)) if explanation_lines else ""
        if not explanation:
            explanation = f"{country_name} {category} ì¹´í…Œê³ ë¦¬ëŠ” í˜„ì¬ ë‹¤ì–‘í•œ í˜ì‹  ì„±ë¶„ê³¼ ê¸°ìˆ ì´ ì£¼ëª©ë°›ê³  ìˆìŠµë‹ˆë‹¤."
        if not key_factors:
            key_factors = [
                "ê³ íš¨ëŠ¥ ì„±ë¶„ì— ëŒ€í•œ ì†Œë¹„ì ê´€ì‹¬ ì¦ê°€",
                "í´ë¦°ë·°í‹° íŠ¸ë Œë“œ í™•ì‚°",
                "SNS ê¸°ë°˜ ë·°í‹° íŠ¸ë Œë“œ ê°€ì†í™”",
                "K-ë·°í‹° ê¸°ìˆ ë ¥ì— ëŒ€í•œ ê¸€ë¡œë²Œ ì‹ ë¢°ë„ ìƒìŠ¹"
            ]

        # Clean up any remaining section markers
        import re as re3
        explanation = re3.sub(r'\[ì„¤ëª…\]|\[í•µì‹¬\s*ìš”ì¸\]', '', explanation).strip()
        key_factors = [re3.sub(r'\[ì„¤ëª…\]|\[í•µì‹¬\s*ìš”ì¸\]', '', f).strip() for f in key_factors]
        key_factors = [f for f in key_factors if len(f) > 3]

        return jsonify({"success": True, "explanation": explanation, "keyFactors": key_factors[:5]})

    except Exception as e:
        print(f"Error in category_trend: {e}")
        return jsonify({"success": False, "error": str(e)}), 500


@app.route("/api/llm/rag-insight", methods=["POST"])
def rag_insight():
    """RAG ê¸°ë°˜ ë§ì¶¤í˜• ì¸ì‚¬ì´íŠ¸ ìƒì„± - ChromaDB ë²¡í„° ê²€ìƒ‰ + EXAONE ì¸ì‚¬ì´íŠ¸"""
    try:
        data = request.json
        scope = data.get("scope", "category")
        insight_type = data.get("type", "marketing")
        keyword = data.get("keyword", "")
        category = data.get("category", "Skincare")
        country = data.get("country", "usa")
        top_keywords = data.get("topKeywords", [])
        positive_reviews = data.get("positiveReviews", [])
        negative_reviews = data.get("negativeReviews", [])

        country_names = {
            "usa": "ë¯¸êµ­", "japan": "ì¼ë³¸", "singapore": "ì‹±ê°€í¬ë¥´",
            "malaysia": "ë§ë ˆì´ì‹œì•„", "indonesia": "ì¸ë„ë„¤ì‹œì•„"
        }
        country_name = country_names.get(country, "í•´ì™¸")

        type_names = {
            "marketing": "ë§ˆì¼€íŒ… ìº í˜ì¸",
            "npd": "ì‹ ì œí’ˆ ê¸°íš(BM)",
            "overseas": "í•´ì™¸ ì§„ì¶œ ì „ëµ"
        }
        type_name = type_names.get(insight_type, "ë§ˆì¼€íŒ… ìº í˜ì¸")

        keywords_text = ""
        if top_keywords:
            keywords_text = ", ".join([k.get("keyword", "") for k in top_keywords[:10]])

        # ===== Vector Search (ìƒˆ ì„ë² ë”© ê¸°ë°˜) =====
        rag_text = ""
        rag_sources = []
        if RAG_AVAILABLE:
            # ì¿¼ë¦¬ êµ¬ì„±: keyword + category + country + topKeywords
            query_parts = []
            if keyword:
                query_parts.append(keyword)
            if category:
                query_parts.append(category)
            if country_name:
                query_parts.append(country_name)
            if top_keywords:
                query_parts.extend([k.get("keyword", "") for k in top_keywords[:5]])
            query_text = " ".join(filter(None, query_parts))

            # RAG ê²€ìƒ‰
            search_results = search_rag(query_text, insight_type, top_k=3)

            if search_results:
                rag_text = "\n\n[ì‹œì¥ ì°¸ê³  ì‚¬ë¡€ - ì‹¤ì œ ë§ˆì¼€íŒ… ì„±ê³µ ì‚¬ë¡€ ë° ì‹œì¥ ì‹ í˜¸ ë°ì´í„°]\n"
                for i, result in enumerate(search_results, 1):
                    doc_type = "ë§ˆì¼€íŒ… ì„±ê³µ ì‚¬ë¡€" if result['type'] == 'marketing_case' else "ì‹œì¥ ì‹ í˜¸"
                    rag_sources.append({
                        "id": result['id'],
                        "brand": result['brand'],
                        "product": result['product'],
                        "type": doc_type
                    })

                    rag_text += f"\nì‚¬ë¡€ {i}: [{doc_type}] {result['brand']} - {result['product']}\n"
                    rag_text += f"  êµ­ê°€: {result['country']} | ì¹´í…Œê³ ë¦¬: {result['category']}\n"

                    if result['type'] == 'marketing_case':
                        if result.get('key_message'):
                            rag_text += f"  í•µì‹¬ ë©”ì‹œì§€: {result['key_message']}\n"
                        if result.get('channel'):
                            rag_text += f"  ì±„ë„: {result['channel']}\n"
                        if result.get('why_it_worked'):
                            rag_text += f"  ì„±ê³µ ìš”ì¸: {result['why_it_worked']}\n"
                        if result.get('evidence_snippet'):
                            rag_text += f"  ê·¼ê±°: {result['evidence_snippet']}\n"
                    else:  # market_signal
                        if result.get('signal_type'):
                            rag_text += f"  ì‹ í˜¸ ìœ í˜•: {result['signal_type']} ({result.get('signal_strength', '')})\n"
                        if result.get('evidence_summary'):
                            rag_text += f"  ê·¼ê±°: {result['evidence_summary']}\n"

                print(f"  RAG: Found {len(search_results)} relevant cases for '{query_text[:50]}...'")

        if scope == "keyword":
            target_desc = f'"{keyword}" í‚¤ì›Œë“œ'
        else:
            target_desc = f'{category} ì¹´í…Œê³ ë¦¬ ì „ì²´ (ì£¼ìš” í‚¤ì›Œë“œ: {keywords_text})'

        # ëª©ì ë³„ í”„ë¡¬í”„íŠ¸
        if insight_type == "marketing":
            # ë§ˆì¼€íŒ…: Query 1 - ì„¹ì…˜ 1, 2, 3ë§Œ ìƒì„±
            purpose_instruction = """ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë§ˆì¼€íŒ… ìº í˜ì¸ ì¸ì‚¬ì´íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”. ë°˜ë“œì‹œ ì•„ë˜ 3ê°œ ì„¹ì…˜ì„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤:

**1. íƒ€ê²Ÿ ì˜¤ë””ì–¸ìŠ¤ ë¶„ì„**
â€¢ **í•µì‹¬ íƒ€ê²Ÿì¸µ:** ì£¼ìš” íƒ€ê²Ÿ ê³ ê°êµ°ì˜ íŠ¹ì„±ê³¼ ë‹ˆì¦ˆë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…
â€¢ **íƒ€ê²Ÿ ì¸ì‚¬ì´íŠ¸:** ì°¸ê³  ì‚¬ë¡€ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œ íš¨ê³¼ì ì¸ íƒ€ê²ŸíŒ… ì „ëµ ì œì•ˆ

**2. ì±„ë„ ë° ì½˜í…ì¸  ì „ëµ**
â€¢ **ì¶”ì²œ ì±„ë„:** ê°€ì¥ íš¨ê³¼ì ì¸ ë§ˆì¼€íŒ… ì±„ë„ (SNS, ì¸í”Œë£¨ì–¸ì„œ, ë¦¬í…Œì¼ ë“±)
â€¢ **ì½˜í…ì¸  ë°©í–¥:** ì°¸ê³  ì‚¬ë¡€ì˜ ì„±ê³µ ì „ëµì„ ë°˜ì˜í•œ ì½˜í…ì¸  ìœ í˜• ë° ë°”ì´ëŸ´ í¬ì¸íŠ¸

**3. í•µì‹¬ ë©”ì‹œì§€ ë° ë¹„ì£¼ì–¼ ì»¨ì…‰**
â€¢ **í•µì‹¬ ë©”ì‹œì§€:** íƒ€ê²Ÿì—ê²Œ ì–´í•„í•  ìˆ˜ ìˆëŠ” ìº í˜ì¸ ìŠ¬ë¡œê±´/ë©”ì‹œì§€
â€¢ **ë¹„ì£¼ì–¼ ë°©í–¥:** íŒ¨í‚¤ì§•, ë¶„ìœ„ê¸°, ìƒ‰ê° ë“± ë¹„ì£¼ì–¼ ì»¨ì…‰ ì œì•ˆ

ìœ„ 3ê°œ ì„¹ì…˜ë§Œ ì‘ì„±í•˜ì„¸ìš”. ê° ì„¹ì…˜ ì œëª©ê³¼ í•˜ìœ„ ì¹´í…Œê³ ë¦¬ëŠ” ë°˜ë“œì‹œ **êµµì€ ê¸€ì”¨**ë¡œ í‘œì‹œí•˜ì„¸ìš”."""

        elif insight_type == "npd":
            purpose_instruction = """ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‹ ì œí’ˆ ê¸°íš ì¸ì‚¬ì´íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”:

Agent Insight

1. ì„±ë¶„ ë°°í•© ì œì•ˆ
ì°¸ê³  ì‚¬ë¡€ì˜ ê³¼í•™ì  ë°°í•© ì¸ì‚¬ì´íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ, í˜„ì¬ íŠ¸ë Œë“œì— ë§ëŠ” ìœ ë§ í•µì‹¬ ì„±ë¶„ ì¡°í•©ê³¼ ê·¸ ê³¼í•™ì  ê·¼ê±°ë¥¼ ì œì•ˆí•´ì£¼ì„¸ìš”.

2. ì œí˜• ì»¨ì…‰ ë° í…ìŠ¤ì²˜
ì°¸ê³  ì‚¬ë¡€ì˜ ì œí˜• í˜ì‹ ì„ ë°”íƒ•ìœ¼ë¡œ, ì†Œë¹„ì ì„ í˜¸ë„ì— ë§ëŠ” ì°¨ë³„í™”ëœ ì œí˜•/í…ìŠ¤ì²˜ì™€ ì „ë‹¬ ì‹œìŠ¤í…œì„ ì œì•ˆí•´ì£¼ì„¸ìš”.

3. USP ë° í¬ì§€ì…”ë‹
ì°¸ê³  ì‚¬ë¡€ì˜ ì‹œì¥ ë°˜ì‘ì„ ë°”íƒ•ìœ¼ë¡œ, ê²½ìŸ ì œí’ˆ ëŒ€ë¹„ ì°¨ë³„í™” í¬ì¸íŠ¸ì™€ ì‹œì¥ í¬ì§€ì…”ë‹ ì „ëµì„ ì œì•ˆí•´ì£¼ì„¸ìš”.

Market Precedent
ì°¸ê³  ì‚¬ë¡€ì—ì„œ ë„ì¶œëœ ì œí˜•/ì„±ë¶„ í˜ì‹  ì„ ë¡€ë¥¼ ë¶ˆë¦¿(â€¢)ìœ¼ë¡œ ì •ë¦¬í•˜ë˜, ê³¼í•™ì  ì‘ìš© ì›ë¦¬ë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”.

Agent Conclusion
ì¢…í•©ì ì¸ ì‹ ì œí’ˆ ê¸°íš ë°©í–¥ì„ 2-3ë¬¸ì¥ìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”."""

        else:
            purpose_instruction = """ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ í•´ì™¸ ì§„ì¶œ ì „ëµ ì¸ì‚¬ì´íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”:

Agent Insight

1. ì‹œì¥ ì§„ì… ì „ëµ
ì°¸ê³  ì‚¬ë¡€ì˜ ì‹¤ì œ ì‹œì¥ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ, í•´ë‹¹ ì‹œì¥ì˜ ìµœì  ì§„ì… ì „ëµ(ìœ í†µ, íƒ€ì´ë°, í¬ì§€ì…”ë‹)ì„ ì œì•ˆí•´ì£¼ì„¸ìš”.

2. í˜„ì§€ ì†Œë¹„ì ë¶„ì„
ì°¸ê³  ì‚¬ë¡€ì˜ ì†Œë¹„ì ì¸ì‚¬ì´íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ, íƒ€ê²Ÿ ì‹œì¥ì˜ ì„ í˜¸ë„, êµ¬ë§¤ íŒ¨í„´, ë¬¸í™”ì  íŠ¹ì„±ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.

3. ìœ í†µ ë° ê°€ê²© ì „ëµ
ì°¸ê³  ì‚¬ë¡€ì˜ ì„±ê³µ/ì‹¤íŒ¨ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì í•©í•œ ìœ í†µ ì±„ë„ê³¼ ê°€ê²© í¬ì§€ì…”ë‹ì„ êµ¬ì²´ì  ìˆ˜ì¹˜ì™€ í•¨ê»˜ ì œì•ˆí•´ì£¼ì„¸ìš”.

Market Precedent
ì°¸ê³  ì‚¬ë¡€ì—ì„œ ë„ì¶œëœ í•´ì™¸ ì§„ì¶œ ì„ ë¡€ì™€ ì„±ê³µ/ì‹¤íŒ¨ ìš”ì¸ì„ ë¶ˆë¦¿(â€¢)ìœ¼ë¡œ ì •ë¦¬í•˜ë˜, êµ¬ì²´ì  ì‹œì¥ ìˆ˜ì¹˜ë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”.

Agent Conclusion
ì¢…í•©ì ì¸ í•´ì™¸ ì§„ì¶œ ì¶”ì²œ ë°©í–¥ì„ 2-3ë¬¸ì¥ìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”."""

        # ë¦¬ë·° ë°ì´í„° ì„¹ì…˜ êµ¬ì„±
        review_section = ""
        if positive_reviews or negative_reviews:
            review_section = "\n[ê³ ê° ë¦¬ë·° ë¶„ì„ - ì‹¤ì œ ì†Œë¹„ì ëª©ì†Œë¦¬]\n"
            if positive_reviews:
                review_section += f"â€¢ ê¸ì • ë¦¬ë·° ì£¼ìš” í‚¤ì›Œë“œ: {', '.join(positive_reviews[:6])}\n"
            if negative_reviews:
                review_section += f"â€¢ ë¶€ì • ë¦¬ë·° ì£¼ìš” í‚¤ì›Œë“œ (ê°œì„  í•„ìš”): {', '.join(negative_reviews[:6])}\n"
            review_section += "â€» íŠ¹íˆ ë¶€ì • ë¦¬ë·°ì—ì„œ ì–¸ê¸‰ëœ ë¶ˆë§Œ ì‚¬í•­ì„ í•´ê²°í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ì „ëµì„ ì œì•ˆí•´ì£¼ì„¸ìš”.\n"

        prompt = f"""ë‹¤ìŒì€ {country_name} ì‹œì¥ì˜ {target_desc}ì— ëŒ€í•œ {type_name} ì¸ì‚¬ì´íŠ¸ ìš”ì²­ì…ë‹ˆë‹¤.

[ë¶„ì„ ëŒ€ìƒ]
- êµ­ê°€: {country_name}
- ì¹´í…Œê³ ë¦¬: {category}
- ë¶„ì„ ë²”ìœ„: {scope} ({keyword if scope == 'keyword' else 'ì¹´í…Œê³ ë¦¬ ì „ì²´'})
- ì£¼ìš” íŠ¸ë Œë“œ í‚¤ì›Œë“œ: {keywords_text}
{review_section}{rag_text}

ìœ„ì˜ ì‹¤ì œ ì‹œì¥ ì°¸ê³  ì‚¬ë¡€ì™€ ê³ ê° ë¦¬ë·° ë°ì´í„°ë¥¼ í•µì‹¬ ê·¼ê±°ë¡œ í™œìš©í•˜ì—¬, ë‹¹ì‹ ì˜ K-ë·°í‹° ì‹œì¥ ì „ë¬¸ ì§€ì‹ê³¼ ê²°í•©í•´ ì‹¤í–‰ ê°€ëŠ¥í•œ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.
ë°˜ë“œì‹œ ì°¸ê³  ì‚¬ë¡€ì˜ êµ¬ì²´ì ì¸ ë°ì´í„°ì™€ ì„±ê³¼ë¥¼ ì¸ìš©í•˜ê³ , ê³ ê° ë¦¬ë·°ì—ì„œ ë„ì¶œëœ ê°œì„ ì ì„ ë°˜ì˜í•˜ì—¬ ì¸ì‚¬ì´íŠ¸ì˜ ì‹¤ìš©ì„±ì„ ë†’ì—¬ì£¼ì„¸ìš”.

{purpose_instruction}"""

        response = generate_response(prompt, max_new_tokens=1200)

        content = clean_text(response) if response else f"{country_name} {category} ì‹œì¥ì— ëŒ€í•œ {type_name} ì¸ì‚¬ì´íŠ¸ì…ë‹ˆë‹¤."

        # ===== ë§ˆì¼€íŒ… íƒ€ì…: Query 2 - ê³¼ê±° ì„±ê³µ ì‚¬ë¡€ (4ë²ˆë§Œ) =====
        query2_section = ""
        if insight_type == "marketing" and rag_sources:
            print("  [Marketing] Query 2: Generating past success cases...")
            query2_prompt = f"""ë‹¹ì‹ ì€ K-ë·°í‹° ë§ˆì¼€íŒ… ì „ëµ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì•„ë˜ ì‹¤ì œ ë§ˆì¼€íŒ… ì„±ê³µ ì‚¬ë¡€ë“¤ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.

[ë¶„ì„ ëŒ€ìƒ]
- êµ­ê°€: {country_name}
- ì¹´í…Œê³ ë¦¬: {category}
- ì£¼ìš” í‚¤ì›Œë“œ: {keywords_text}

{rag_text}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì •í™•íˆ ì‘ì„±í•´ì£¼ì„¸ìš”:

**4. ê³¼ê±° ì„±ê³µ ì‚¬ë¡€ ë¶„ì„**

**4-1. [ë¸Œëœë“œëª… - ì œí’ˆëª…]**
â€¢ <ì„±ê³¼ì§€í‘œ> í•´ë‹¹ ì‚¬ë¡€ì˜ êµ¬ì²´ì ì¸ ì„±ê³µ ìˆ˜ì¹˜ (ë§¤ì¶œ ì¦ê°€ìœ¨, íŒë§¤ëŸ‰, ì¸ì§€ë„ ìƒìŠ¹ ë“±)
â€¢ <í•µì‹¬ì „ëµ> ì´ ì‚¬ë¡€ì—ì„œ ë°°ìš¸ ìˆ˜ ìˆëŠ” í•µì‹¬ ì„±ê³µ ì „ëµ
â€¢ <ì ìš©ë°©ì•ˆ> {target_desc}ì— ì´ ì „ëµì„ ì–´ë–»ê²Œ ì ìš©í•  ìˆ˜ ìˆëŠ”ì§€

**4-2. [ë¸Œëœë“œëª… - ì œí’ˆëª…]**
â€¢ <ì„±ê³¼ì§€í‘œ> í•´ë‹¹ ì‚¬ë¡€ì˜ êµ¬ì²´ì ì¸ ì„±ê³µ ìˆ˜ì¹˜
â€¢ <í•µì‹¬ì „ëµ> ì´ ì‚¬ë¡€ì—ì„œ ë°°ìš¸ ìˆ˜ ìˆëŠ” í•µì‹¬ ì„±ê³µ ì „ëµ
â€¢ <ì ìš©ë°©ì•ˆ> {target_desc}ì— ì´ ì „ëµì„ ì–´ë–»ê²Œ ì ìš©í•  ìˆ˜ ìˆëŠ”ì§€

**4-3. [ë¸Œëœë“œëª… - ì œí’ˆëª…]**
â€¢ <ì„±ê³¼ì§€í‘œ> í•´ë‹¹ ì‚¬ë¡€ì˜ êµ¬ì²´ì ì¸ ì„±ê³µ ìˆ˜ì¹˜
â€¢ <í•µì‹¬ì „ëµ> ì´ ì‚¬ë¡€ì—ì„œ ë°°ìš¸ ìˆ˜ ìˆëŠ” í•µì‹¬ ì„±ê³µ ì „ëµ
â€¢ <ì ìš©ë°©ì•ˆ> {target_desc}ì— ì´ ì „ëµì„ ì–´ë–»ê²Œ ì ìš©í•  ìˆ˜ ìˆëŠ”ì§€

ë°˜ë“œì‹œ ìœ„ í˜•ì‹ì„ ì¤€ìˆ˜í•˜ì„¸ìš”. 4-1, 4-2, 4-3 ì œëª©ì€ **êµµì€ ê¸€ì”¨**ë¡œ í‘œì‹œí•˜ê³ , í•˜ìœ„ í•­ëª©ì€ <ì„±ê³¼ì§€í‘œ>, <í•µì‹¬ì „ëµ>, <ì ìš©ë°©ì•ˆ> í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.
Agent InsightëŠ” ì‘ì„±í•˜ì§€ ë§ˆì„¸ìš”."""

            query2_response = generate_response(query2_prompt, max_new_tokens=800)
            if query2_response:
                # ë§ˆí¬ë‹¤ìš´ ìœ ì§€ (** êµµì€ê¸€ì”¨, <> êµ¬ë¶„ì)
                query2_section = "\n\n" + query2_response.strip()
                print("  [Marketing] Query 2 completed successfully")

        # ===== ë§ˆì¼€íŒ… íƒ€ì…: Query 3 - Agent Insight (ì¢…í•© ì „ëµ ìš”ì•½) =====
        agent_insight_content = ""
        if insight_type == "marketing":
            print("  [Marketing] Query 3: Generating Agent Insight summary...", flush=True)

            # ì´ì „ ì¿¼ë¦¬ ê²°ê³¼ë¥¼ ìºì‹œë¡œ ì‚¬ìš© (1~4ë²ˆ ë‚´ìš©)
            cached_analysis = content.strip()
            if query2_section:
                cached_analysis += query2_section

            query3_prompt = f"""ë‹¹ì‹ ì€ K-ë·°í‹° ë§ˆì¼€íŒ… ì „ëµ ìˆ˜ì„ ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤.

[ë¶„ì„ ëŒ€ìƒ]
- êµ­ê°€: {country_name}
- ì¹´í…Œê³ ë¦¬: {category}
- íƒ€ê²Ÿ: {target_desc}

[ì´ì „ ë¶„ì„ ë‚´ìš© ìš”ì•½]
{cached_analysis}

ìœ„ ë¶„ì„ ë‚´ìš©(íƒ€ê²Ÿ ì˜¤ë””ì–¸ìŠ¤, ì±„ë„ ì „ëµ, í•µì‹¬ ë©”ì‹œì§€, ê³¼ê±° ì„±ê³µ ì‚¬ë¡€)ì„ ì¢…í•©í•˜ì—¬ ìµœì¢… ë§ˆì¼€íŒ… ì „ëµì„ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”.

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì •í™•íˆ ì‘ì„±í•´ì£¼ì„¸ìš” (5-6ë¬¸ì¥ ì´ë‚´ë¡œ í•µì‹¬ë§Œ):

{country_name} {category} ì‹œì¥ì—ì„œ {target_desc}ì„ ìœ„í•œ ìµœì¢… ë§ˆì¼€íŒ… ì „ëµì…ë‹ˆë‹¤.

**í•µì‹¬ íƒ€ê²Ÿ** ì§‘ì¤‘í•´ì•¼ í•  íƒ€ê²Ÿì¸µ (1ë¬¸ì¥)
**ì¶”ì²œ ì±„ë„** ìµœìš°ì„  ë§ˆì¼€íŒ… ì±„ë„ (1ë¬¸ì¥)
**í•µì‹¬ ë©”ì‹œì§€** ìº í˜ì¸ í•µì‹¬ ë©”ì‹œì§€/ì»¨ì…‰ (1ë¬¸ì¥)
**ë²¤ì¹˜ë§ˆí¬** ê³¼ê±° ì„±ê³µ ì‚¬ë¡€ì—ì„œ ë°°ìš´ í•µì‹¬ í¬ì¸íŠ¸ (1ë¬¸ì¥)
**ì‹¤í–‰ ì•¡ì…˜** ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥í•œ ì•¡ì…˜ (1ë¬¸ì¥)

ê°„ê²°í•˜ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ í˜•íƒœë¡œ ì‘ì„±í•˜ì„¸ìš”. ê° í•­ëª©ì€ **í•­ëª©ëª…** í˜•ì‹ìœ¼ë¡œ ì‹œì‘í•˜ê³  ì½œë¡ (:)ì€ ë„£ì§€ ë§ˆì„¸ìš”."""

            query3_response = generate_response(query3_prompt, max_new_tokens=400)
            if query3_response:
                # Agent InsightëŠ” ë§ˆí¬ë‹¤ìš´(**êµµì€ê¸€ì”¨**) ìœ ì§€ - í”„ë¡ íŠ¸ì—”ë“œì—ì„œ íŒŒì‹±í•¨
                agent_insight_content = query3_response.strip()
                print(f"  [Marketing] Query 3 (Agent Insight) completed: {len(agent_insight_content)} chars", flush=True)
            else:
                print("  [Marketing] Query 3 FAILED - no response!", flush=True)

        # ì½˜í…ì¸  ì¡°í•© (Agent InsightëŠ” ë³„ë„ í•„ë“œë¡œ ë°˜í™˜)
        if insight_type == "marketing":
            # Query 1 (1, 2, 3ë²ˆ) + Query 2 (4ë²ˆ) - Agent InsightëŠ” ë³„ë„
            content = content.strip() + query2_section
        else:
            if "Agent Insight" not in content:
                content = f"Agent Insight\n\n{content}"
            if "Agent Conclusion" not in content:
                content += "\n\nAgent Conclusion\n\nìœ„ ë¶„ì„ì„ ì¢…í•©í•˜ë©´, í˜„ì¬ ì‹œì¥ íŠ¸ë Œë“œì™€ ì‹¤ì œ ì‚¬ë¡€ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì „ëµì  ì ‘ê·¼ì´ í•„ìš”í•©ë‹ˆë‹¤."

        return jsonify({
            "success": True,
            "content": content,
            "agentInsight": agent_insight_content if insight_type == "marketing" else "",
            "scope": scope,
            "type": insight_type,
            "keyword": keyword,
            "category": category,
            "country": country,
            "ragSources": rag_sources,
        })

    except Exception as e:
        print(f"Error in rag_insight: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({"success": False, "error": str(e)}), 500


@app.route("/api/llm/plc-prediction", methods=["POST"])
def plc_prediction():
    """ë‹¤ì¤‘ í”„ë ˆì„ì›Œí¬ ê¸°ë°˜ í–¥í›„ 6-12ê°œì›” ì˜ˆì¸¡ (PLC + Trend Diffusion + Consumer Demand)"""
    try:
        data = request.json
        keyword = data.get("keyword", "")
        trend_level = data.get("trendLevel", "Actionable")
        current_score = data.get("currentScore", 75)
        sns_growth = data.get("snsGrowth", 30)
        retail_signal = data.get("retailSignal", 70)
        category = data.get("category", "Skincare")

        prompt = f"""ë‹¹ì‹ ì€ ë·°í‹° í‚¤ì›Œë“œ íŠ¸ë Œë“œë¥¼ ì˜ˆì¸¡í•˜ëŠ” ì‹œë‹ˆì–´ ì• ë„ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.
ë‹¨ìˆœ PLC(Product Life Cycle) ì´ë¡ ë§Œìœ¼ë¡œ íŒë‹¨í•˜ì§€ ë§ê³ , ì•„ë˜ 3ê°€ì§€ í”„ë ˆì„ì„ í•¨ê»˜ ì‚¬ìš©í•´ ë³µí•©ì ìœ¼ë¡œ ì˜ˆì¸¡í•˜ì„¸ìš”.

[í”„ë ˆì„ A: PLC ë‹¨ê³„(ê¸°ë³¸)]
- ë„ì…ê¸°: ì–¼ë¦¬ì–´ë‹µí„° ì¤‘ì‹¬, ë‚®ì€ ì¸ì§€ë„, í˜ì‹  ì†Œë¹„ì íƒ€ê²Ÿ
- ì„±ì¥ê¸°: ë¹ ë¥¸ í™•ì‚°, SNS ë°”ì´ëŸ´, ì‹œì¥ ì ìœ ìœ¨ í™•ëŒ€, ê²½ìŸì ì§„ì… ì‹œì‘
- ì„±ìˆ™ê¸°: ëŒ€ì¤‘í™” ì™„ë£Œ, ì„±ì¥ë¥  ë‘”í™”, ê²½ìŸ ì‹¬í™”, ì•ˆì •ì  ìˆ˜ìš”
- ì‡ í‡´ê¸°: ê´€ì‹¬ ê°ì†Œ, ìƒˆë¡œìš´ íŠ¸ë Œë“œë¡œ ëŒ€ì²´, ë‹ˆì¹˜ ì‹œì¥ìœ¼ë¡œ ì¶•ì†Œ

[í”„ë ˆì„ B: í™•ì‚°/ê°€ì†(Trend Diffusion & Momentum)]
- ì£¼ìš” í‚¤ì›Œë“œê°€ 'ë‹¨ë°œì„± ë²„ì¦ˆ'ì¸ì§€ 'ë£¨í‹´í™”/ì‚¬ìš© ë§¥ë½ í™•ì¥'ì¸ì§€ êµ¬ë¶„
- SNS ë°˜ì‘ê³¼ ë¦¬í…Œì¼ ë°˜ì‘ì´ í•¨ê»˜ ì›€ì§ì´ë©´ ìˆ˜ìš”í˜•(ì§€ì†), SNSë§Œ ê³¼ì—´ì´ë©´ ë²„ì¦ˆí˜•(ë‹¨ê¸°)
- ì¸í”Œë£¨ì–¸ì„œ ì£¼ë„ vs ì¼ë°˜ ì†Œë¹„ì í™•ì‚° ì—¬ë¶€ íŒë‹¨

[í”„ë ˆì„ C: ìˆ˜ìš” ì•ˆì •ì„± + ë¦¬ìŠ¤í¬(Consumer Demand & Risk)]
- í‚¤ì›Œë“œê°€ ìƒì‹œ ê³ ë¯¼(ì¥ë²½/ì§„ì •/ì—¬ë“œë¦„/ë³´ìŠµ ë“±) ê¸°ë°˜ì¸ì§€, ì‹œì¦Œ/ìœ í–‰ ê¸°ë°˜ì¸ì§€ í‰ê°€
- ìê·¹/ë¶ˆë§Œ/í”¼ë¡œê°(ê³¼ê°ì§ˆ/ê³¼ìê·¹) ì´ìŠˆê°€ ì»¤ì§€ëŠ”ì§€ ê³ ë ¤
- ê²½ìŸ/ëŒ€ì²´ì¬ ì¶œí˜„ ì†ë„ë¥¼ ê³ ë ¤ (ê¸‰ê²©í•œ ì„±ìˆ™/í•˜ë½ ê°€ëŠ¥)
- ê°€ê²©ëŒ€ë¹„ íš¨ê³¼ ì¸ì‹ ë³€í™” ì¶”ì 

[í‚¤ì›Œë“œ ë°ì´í„°]
- í‚¤ì›Œë“œ: {keyword}
- íŠ¸ë Œë“œ ë ˆë²¨: {trend_level}
- í˜„ì¬ ì¢…í•© ì ìˆ˜: {current_score}/100
- SNS ì„±ì¥ë¥ : {sns_growth}%
- ë¦¬í…Œì¼ ì‹ í˜¸ ê°•ë„: {retail_signal}%
- ì¹´í…Œê³ ë¦¬: {category}

íŒì • ê·œì¹™(ë°˜ë“œì‹œ ì¤€ìˆ˜):
1) ì ìˆ˜ í•˜ë‚˜ë¡œ ë‹¨ê³„ ê²°ì • ê¸ˆì§€ - í‚¤ì›Œë“œ íŠ¹ì„±, í™•ì‚°/ë¦¬í…Œì¼ ê· í˜•, ë¦¬ìŠ¤í¬ ëª¨ë‘ ë°˜ì˜
2) ì›”ë³„ì ìˆ˜ëŠ” í•­ìƒ ë‹¨ì¡° ì¦ê°€/ê°ì†Œ ê¸ˆì§€ - ë³€ê³¡ì /í”¼í¬/ì•ˆì •í™” êµ¬ê°„ì„ í˜„ì‹¤ì ìœ¼ë¡œ ë°˜ì˜
3) SNSë§Œ ë†’ê³  Retail ë‚®ìœ¼ë©´ ë²„ì¦ˆ ì†Œë©¸ ë¦¬ìŠ¤í¬, ë‘˜ ë‹¤ ë†’ìœ¼ë©´ ì§€ì† ì„±ì¥ ê°€ëŠ¥ì„± ë†’ìŒ

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì •í™•íˆ ë‹µë³€í•´ì£¼ì„¸ìš”:

[í˜„ì¬ë‹¨ê³„] ë„ì…ê¸°/ì„±ì¥ê¸°/ì„±ìˆ™ê¸°/ì‡ í‡´ê¸° ì¤‘ í•˜ë‚˜
[6ê°œì›”ì˜ˆì¸¡] ë„ì…ê¸°/ì„±ì¥ê¸°/ì„±ìˆ™ê¸°/ì‡ í‡´ê¸° ì¤‘ í•˜ë‚˜
[12ê°œì›”ì˜ˆì¸¡] ë„ì…ê¸°/ì„±ì¥ê¸°/ì„±ìˆ™ê¸°/ì‡ í‡´ê¸° ì¤‘ í•˜ë‚˜
[ì›”ë³„ì ìˆ˜] í˜„ì¬ë¶€í„° 12ê°œì›” í›„ê¹Œì§€ 13ê°œì˜ ì˜ˆì¸¡ ì ìˆ˜ë¥¼ ì‰¼í‘œë¡œ êµ¬ë¶„ (0-100 ë²”ìœ„)
[ë¶„ì„] 3-5ë¬¸ì¥ìœ¼ë¡œ ë‹¤ìŒ í¬í•¨: (a)ì„±ì¥/ì§€ì† ë“œë¼ì´ë²„ (b)í•˜ë½/ì†Œë©¸ ë¦¬ìŠ¤í¬ (c)ì¡°ê±´ë¶€ ì‹œë‚˜ë¦¬ì˜¤"""

        response = generate_response(prompt, max_new_tokens=600)

        current_phase = "ì„±ì¥ê¸°"
        prediction_6m = "ì„±ìˆ™ê¸°"
        prediction_12m = "ì„±ìˆ™ê¸°"
        monthly_scores = []
        explanation = ""

        lines = response.split("\n")
        explanation_lines = []
        current_section = None

        for line in lines:
            line = line.strip()
            if not line:
                continue

            if any(kw in line for kw in ["[í˜„ì¬ë‹¨ê³„]", "í˜„ì¬ë‹¨ê³„:", "í˜„ì¬ ë‹¨ê³„"]):
                for phase in ["ë„ì…ê¸°", "ì„±ì¥ê¸°", "ì„±ìˆ™ê¸°", "ì‡ í‡´ê¸°"]:
                    if phase in line:
                        current_phase = phase
                        break
            elif any(kw in line for kw in ["[6ê°œì›”ì˜ˆì¸¡]", "6ê°œì›”ì˜ˆì¸¡:", "6ê°œì›” ì˜ˆì¸¡"]):
                for phase in ["ë„ì…ê¸°", "ì„±ì¥ê¸°", "ì„±ìˆ™ê¸°", "ì‡ í‡´ê¸°"]:
                    if phase in line:
                        prediction_6m = phase
                        break
            elif any(kw in line for kw in ["[12ê°œì›”ì˜ˆì¸¡]", "12ê°œì›”ì˜ˆì¸¡:", "12ê°œì›” ì˜ˆì¸¡"]):
                for phase in ["ë„ì…ê¸°", "ì„±ì¥ê¸°", "ì„±ìˆ™ê¸°", "ì‡ í‡´ê¸°"]:
                    if phase in line:
                        prediction_12m = phase
                        break
            elif any(kw in line for kw in ["[ì›”ë³„ì ìˆ˜]", "ì›”ë³„ì ìˆ˜:", "ì›”ë³„ ì ìˆ˜"]):
                numbers = re.findall(r'\d+(?:\.\d+)?', line.split("]")[-1] if "]" in line else line.split(":")[-1])
                monthly_scores = [min(100, max(0, float(n))) for n in numbers[:13]]
                current_section = "scores"
            elif any(kw in line for kw in ["[ë¶„ì„]", "ë¶„ì„:", "## ë¶„ì„"]):
                current_section = "explanation"
                rest = re.sub(r'^\[ë¶„ì„\]|^ë¶„ì„:|^## ë¶„ì„', '', line).strip()
                if rest and len(rest) > 5:
                    explanation_lines.append(rest)
            elif current_section == "scores" and not monthly_scores:
                numbers = re.findall(r'\d+(?:\.\d+)?', line)
                if numbers:
                    monthly_scores = [min(100, max(0, float(n))) for n in numbers[:13]]
            elif current_section == "explanation":
                clean_line = line.lstrip("0123456789.-â€¢â†’Â·)#* ").strip()
                if clean_line and len(clean_line) > 5:
                    explanation_lines.append(clean_line)

        explanation = clean_text(" ".join(explanation_lines)) if explanation_lines else ""

        if len(monthly_scores) < 13:
            base = current_score
            phase_map = {"ë„ì…ê¸°": "growth", "ì„±ì¥ê¸°": "peak", "ì„±ìˆ™ê¸°": "stable", "ì‡ í‡´ê¸°": "decline"}
            trend = phase_map.get(current_phase, "stable")

            monthly_scores = [base]
            for i in range(1, 13):
                if trend == "growth":
                    delta = 2.5 + (i * 0.3)
                elif trend == "peak":
                    delta = 1.5 - (i * 0.2) if i < 6 else -0.5
                elif trend == "stable":
                    delta = 0.5 - (i * 0.1)
                else:
                    delta = -1.5 - (i * 0.3)
                monthly_scores.append(min(100, max(10, monthly_scores[-1] + delta)))

        if not explanation:
            explanation = f"{keyword}ëŠ” í˜„ì¬ {current_phase} ë‹¨ê³„ì— ìˆìœ¼ë©°, 6ê°œì›” í›„ {prediction_6m}, 12ê°œì›” í›„ {prediction_12m} ë‹¨ê³„ë¡œ ì§„í–‰ë  ê²ƒìœ¼ë¡œ ì˜ˆì¸¡ë©ë‹ˆë‹¤."

        return jsonify({
            "success": True,
            "currentPhase": current_phase,
            "prediction6m": prediction_6m,
            "prediction12m": prediction_12m,
            "monthlyScores": monthly_scores[:13],
            "explanation": explanation,
        })

    except Exception as e:
        print(f"Error in plc_prediction: {e}")
        return jsonify({"success": False, "error": str(e)}), 500


@app.route("/api/llm/category-prediction", methods=["POST"])
def category_prediction():
    """ì¹´í…Œê³ ë¦¬ ì „ì²´ì˜ ë‹¤ì¤‘ í”„ë ˆì„ì›Œí¬ ê¸°ë°˜ í–¥í›„ 6-12ê°œì›” ì˜ˆì¸¡"""
    try:
        data = request.json
        country = data.get("country", "usa")
        category = data.get("category", "Skincare")
        top_keywords = data.get("topKeywords", [])
        avg_score = data.get("avgScore", 70)

        country_names = {
            "usa": "ë¯¸êµ­", "japan": "ì¼ë³¸", "singapore": "ì‹±ê°€í¬ë¥´",
            "malaysia": "ë§ë ˆì´ì‹œì•„", "indonesia": "ì¸ë„ë„¤ì‹œì•„"
        }
        country_name = country_names.get(country, "í•´ì™¸")

        keywords_summary = ""
        if top_keywords:
            keywords_summary = ", ".join([f"{k.get('keyword', '')}({k.get('score', 0)}ì )" for k in top_keywords[:10]])

        prompt = f"""ë‹¹ì‹ ì€ ë·°í‹° ì¹´í…Œê³ ë¦¬ íŠ¸ë Œë“œë¥¼ ì˜ˆì¸¡í•˜ëŠ” ì‹œë‹ˆì–´ ì• ë„ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.
ë‹¨ìˆœ PLC(Product Life Cycle) ì´ë¡ ë§Œìœ¼ë¡œ íŒë‹¨í•˜ì§€ ë§ê³ , ì•„ë˜ 3ê°€ì§€ í”„ë ˆì„ì„ í•¨ê»˜ ì‚¬ìš©í•´ ë³µí•©ì ìœ¼ë¡œ ì˜ˆì¸¡í•˜ì„¸ìš”.

[í”„ë ˆì„ A: PLC ë‹¨ê³„(ê¸°ë³¸)]
- ë„ì…ê¸°: ì–¼ë¦¬ì–´ë‹µí„° ì¤‘ì‹¬ ê´€ì‹¬, í˜ì‹ ì  í‚¤ì›Œë“œ ë“±ì¥
- ì„±ì¥ê¸°: ë¹ ë¥¸ í™•ì‚°, SNS ë°”ì´ëŸ´, ì‹œì¥ ì ìœ ìœ¨ í™•ëŒ€
- ì„±ìˆ™ê¸°: ëŒ€ì¤‘í™” ì™„ë£Œ, ì„±ì¥ë¥  ë‘”í™”, ì•ˆì •ì  ìˆ˜ìš”
- ì‡ í‡´ê¸°: ê´€ì‹¬ ê°ì†Œ, ìƒˆë¡œìš´ íŠ¸ë Œë“œë¡œ ëŒ€ì²´

[í”„ë ˆì„ B: í™•ì‚°/ê°€ì†(Trend Diffusion & Momentum)]
- ì¹´í…Œê³ ë¦¬ ë‚´ ì£¼ìš” í‚¤ì›Œë“œë“¤ì´ 'ë‹¨ë°œì„± ë²„ì¦ˆ'ì¸ì§€ 'ë£¨í‹´í™”/ì‚¬ìš© ë§¥ë½ í™•ì¥'ì¸ì§€ êµ¬ë¶„
- SNS ë°˜ì‘ê³¼ ë¦¬í…Œì¼ ë°˜ì‘ì´ í•¨ê»˜ ì›€ì§ì´ë©´ ìˆ˜ìš”í˜•(ì§€ì†), SNSë§Œ ê³¼ì—´ì´ë©´ ë²„ì¦ˆí˜•(ë‹¨ê¸°)
- ì¹´í…Œê³ ë¦¬ ë‚´ ë‹¤ì–‘í•œ í‚¤ì›Œë“œì˜ íŠ¸ë Œë“œ ë ˆë²¨ ë¶„í¬ ê³ ë ¤ (Emerging ë‹¤ìˆ˜ vs Actionable/Mature ë‹¤ìˆ˜)

[í”„ë ˆì„ C: ìˆ˜ìš” ì•ˆì •ì„± + ë¦¬ìŠ¤í¬(Consumer Demand & Risk)]
- ì¹´í…Œê³ ë¦¬ê°€ ìƒì‹œ ê³ ë¯¼(ì¥ë²½/ì§„ì •/ì—¬ë“œë¦„/ë³´ìŠµ ë“±) ê¸°ë°˜ì¸ì§€, ì‹œì¦Œ/ìœ í–‰ ê¸°ë°˜ì¸ì§€ í‰ê°€
- ìê·¹/ë¶ˆë§Œ/í”¼ë¡œê°(ê³¼ê°ì§ˆ/ê³¼ìê·¹) ì´ìŠˆê°€ ì»¤ì§€ëŠ”ì§€ ê³ ë ¤
- ê²½ìŸ/ëŒ€ì²´ì¬ ì¶œí˜„ ì†ë„ë¥¼ ê³ ë ¤ (ê¸‰ê²©í•œ ì„±ìˆ™/í•˜ë½ ê°€ëŠ¥)
- ê·œì œ í™˜ê²½ ë³€í™”(ì„±ë¶„ ê·œì œ, í´ë¦°ë·°í‹° ê¸°ì¤€ ë“±) ë¦¬ìŠ¤í¬ ë°˜ì˜

[ì¹´í…Œê³ ë¦¬ ë°ì´í„°]
- êµ­ê°€: {country_name}
- ì¹´í…Œê³ ë¦¬: {category}
- ì¹´í…Œê³ ë¦¬ í‰ê·  ì ìˆ˜: {avg_score}/100
- ì£¼ìš” í‚¤ì›Œë“œ: {keywords_summary}

íŒì • ê·œì¹™(ë°˜ë“œì‹œ ì¤€ìˆ˜):
1) ì ìˆ˜ í•˜ë‚˜ë¡œ ë‹¨ê³„ ê²°ì • ê¸ˆì§€ - í‚¤ì›Œë“œ êµ¬ì„±, í™•ì‚°/ë¦¬í…Œì¼ ê· í˜•, ë¦¬ìŠ¤í¬ ë°˜ì˜
2) ì›”ë³„ì ìˆ˜ëŠ” í•­ìƒ ë‹¨ì¡° ì¦ê°€/ê°ì†Œ ê¸ˆì§€ - ë³€ê³¡/í”¼í¬ ê°€ëŠ¥ì„± ë°˜ì˜
3) êµ­ê°€ë³„ ì‹œì¥ íŠ¹ì„±(ë¯¸êµ­=ì„±ë¶„ì£¼ì˜, ì¼ë³¸=í…ìŠ¤ì²˜/ê¸°ëŠ¥, ë™ë‚¨ì•„=ê¸°ì´ˆìŠ¤í‚¨ì¼€ì–´) ë°˜ì˜

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì •í™•íˆ ë‹µë³€í•´ì£¼ì„¸ìš”:

[í˜„ì¬ë‹¨ê³„] ë„ì…ê¸°/ì„±ì¥ê¸°/ì„±ìˆ™ê¸°/ì‡ í‡´ê¸° ì¤‘ í•˜ë‚˜
[6ê°œì›”ì˜ˆì¸¡] ë„ì…ê¸°/ì„±ì¥ê¸°/ì„±ìˆ™ê¸°/ì‡ í‡´ê¸° ì¤‘ í•˜ë‚˜
[12ê°œì›”ì˜ˆì¸¡] ë„ì…ê¸°/ì„±ì¥ê¸°/ì„±ìˆ™ê¸°/ì‡ í‡´ê¸° ì¤‘ í•˜ë‚˜
[ì›”ë³„ì ìˆ˜] í˜„ì¬ë¶€í„° 12ê°œì›” í›„ê¹Œì§€ 13ê°œì˜ ì˜ˆì¸¡ ì ìˆ˜ë¥¼ ì‰¼í‘œë¡œ êµ¬ë¶„ (0-100 ë²”ìœ„)
[ë¶„ì„] 3-5ë¬¸ì¥ìœ¼ë¡œ ë‹¤ìŒ í¬í•¨: (a)ì„±ì¥/ì§€ì† ë“œë¼ì´ë²„ (b)í•˜ë½/ì†Œë©¸ ë¦¬ìŠ¤í¬ (c)ì¡°ê±´ë¶€ ì‹œë‚˜ë¦¬ì˜¤"""

        response = generate_response(prompt, max_new_tokens=600)

        current_phase = "ì„±ì¥ê¸°"
        prediction_6m = "ì„±ìˆ™ê¸°"
        prediction_12m = "ì„±ìˆ™ê¸°"
        monthly_scores = []
        explanation = ""

        lines = response.split("\n")
        explanation_lines = []
        current_section = None

        for line in lines:
            line = line.strip()
            if not line:
                continue

            if any(kw in line for kw in ["[í˜„ì¬ë‹¨ê³„]", "í˜„ì¬ë‹¨ê³„:", "í˜„ì¬ ë‹¨ê³„"]):
                for phase in ["ë„ì…ê¸°", "ì„±ì¥ê¸°", "ì„±ìˆ™ê¸°", "ì‡ í‡´ê¸°"]:
                    if phase in line:
                        current_phase = phase
                        break
            elif any(kw in line for kw in ["[6ê°œì›”ì˜ˆì¸¡]", "6ê°œì›”ì˜ˆì¸¡:", "6ê°œì›” ì˜ˆì¸¡"]):
                for phase in ["ë„ì…ê¸°", "ì„±ì¥ê¸°", "ì„±ìˆ™ê¸°", "ì‡ í‡´ê¸°"]:
                    if phase in line:
                        prediction_6m = phase
                        break
            elif any(kw in line for kw in ["[12ê°œì›”ì˜ˆì¸¡]", "12ê°œì›”ì˜ˆì¸¡:", "12ê°œì›” ì˜ˆì¸¡"]):
                for phase in ["ë„ì…ê¸°", "ì„±ì¥ê¸°", "ì„±ìˆ™ê¸°", "ì‡ í‡´ê¸°"]:
                    if phase in line:
                        prediction_12m = phase
                        break
            elif any(kw in line for kw in ["[ì›”ë³„ì ìˆ˜]", "ì›”ë³„ì ìˆ˜:", "ì›”ë³„ ì ìˆ˜"]):
                numbers = re.findall(r'\d+(?:\.\d+)?', line.split("]")[-1] if "]" in line else line.split(":")[-1])
                monthly_scores = [min(100, max(0, float(n))) for n in numbers[:13]]
                current_section = "scores"
            elif any(kw in line for kw in ["[ë¶„ì„]", "ë¶„ì„:", "## ë¶„ì„"]):
                current_section = "explanation"
                rest = re.sub(r'^\[ë¶„ì„\]|^ë¶„ì„:|^## ë¶„ì„', '', line).strip()
                if rest and len(rest) > 5:
                    explanation_lines.append(rest)
            elif current_section == "scores" and not monthly_scores:
                numbers = re.findall(r'\d+(?:\.\d+)?', line)
                if numbers:
                    monthly_scores = [min(100, max(0, float(n))) for n in numbers[:13]]
            elif current_section == "explanation":
                clean_line = line.lstrip("0123456789.-â€¢â†’Â·)#* ").strip()
                if clean_line and len(clean_line) > 5:
                    explanation_lines.append(clean_line)

        explanation = clean_text(" ".join(explanation_lines)) if explanation_lines else ""

        if len(monthly_scores) < 13:
            base = avg_score
            phase_map = {"ë„ì…ê¸°": "growth", "ì„±ì¥ê¸°": "peak", "ì„±ìˆ™ê¸°": "stable", "ì‡ í‡´ê¸°": "decline"}
            trend = phase_map.get(current_phase, "stable")

            monthly_scores = [base]
            for i in range(1, 13):
                if trend == "growth":
                    delta = 2.0 + (i * 0.25)
                elif trend == "peak":
                    delta = 1.2 - (i * 0.15) if i < 6 else -0.3
                elif trend == "stable":
                    delta = 0.3 - (i * 0.05)
                else:
                    delta = -1.2 - (i * 0.2)
                monthly_scores.append(min(100, max(10, monthly_scores[-1] + delta)))

        if not explanation:
            explanation = f"{country_name} {category} ì¹´í…Œê³ ë¦¬ëŠ” í˜„ì¬ {current_phase} ë‹¨ê³„ì— ìˆìœ¼ë©°, 6ê°œì›” í›„ {prediction_6m}, 12ê°œì›” í›„ {prediction_12m} ë‹¨ê³„ë¡œ ì§„í–‰ë  ê²ƒìœ¼ë¡œ ì˜ˆì¸¡ë©ë‹ˆë‹¤."

        return jsonify({
            "success": True,
            "currentPhase": current_phase,
            "prediction6m": prediction_6m,
            "prediction12m": prediction_12m,
            "monthlyScores": monthly_scores[:13],
            "explanation": explanation,
        })

    except Exception as e:
        print(f"Error in category_prediction: {e}")
        return jsonify({"success": False, "error": str(e)}), 500


@app.route("/api/llm/health", methods=["GET"])
def health_check():
    """Health check endpoint"""
    return jsonify({"status": "ok", "model": MODEL_NAME, "device": DEVICE})


# ===== Chat Endpoint for Text-only Chatbot (EXAONE) =====

CHAT_SYSTEM_PROMPT = """ë‹¹ì‹ ì€ AMORE CLUE ëŒ€ì‹œë³´ë“œì˜ K-ë·°í‹° íŠ¸ë Œë“œ ë¶„ì„ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

## ì—­í• 
- ê¸€ë¡œë²Œ K-ë·°í‹°(K-Beauty) ì‹œì¥ íŠ¸ë Œë“œ ë¶„ì„ ì „ë¬¸ê°€
- í™”ì¥í’ˆ ì‚°ì—… ë°ì´í„° ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸ ì œê³µ
- ì•„ëª¨ë ˆí¼ì‹œí”½, LGìƒí™œê±´ê°• ë“± í•œêµ­ í™”ì¥í’ˆ ê¸°ì—…ì˜ ê¸€ë¡œë²Œ ì „ëµ ìë¬¸ ìˆ˜ì¤€ì˜ ì „ë¬¸ì„±

## ë°ì´í„° ì†ŒìŠ¤
ë‹¹ì‹ ì—ê²Œ ì œê³µë˜ëŠ” [DB ë°ì´í„°]ëŠ” ì‹¤ì œ ì‹œì¥ì—ì„œ ìˆ˜ì§‘ëœ ë°ì´í„°ì…ë‹ˆë‹¤:
- ë¦¬ë”ë³´ë“œ: í˜„ì¬ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ìˆœìœ„ ë° ì ìˆ˜
- SNS ë°ì´í„°: ì¸ìŠ¤íƒ€ê·¸ë¨, í‹±í†¡, ë ˆë”§ ë“± í”Œë«í¼ë³„ ë²„ì¦ˆëŸ‰
- ë¦¬ë·° ë¶„ì„: ì•„ë§ˆì¡´ ë“± ì‹¤ì œ ì†Œë¹„ì ë¦¬ë·° ê°ì„± ë¶„ì„
- íŠ¸ë Œë“œ ì¡°í•©: ì„±ë¶„+ì œí˜•+íš¨ê³¼ì˜ ì¸ê¸° ì¡°í•©

## ë‹µë³€ ê·œì¹™
1. ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”
2. ì œê³µëœ [DB ë°ì´í„°]ë¥¼ í•µì‹¬ ê·¼ê±°ë¡œ í™œìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”
3. ë‹µë³€ì€ êµ¬ì¡°í™”í•˜ì—¬ ì œê³µí•˜ì„¸ìš” (ì†Œì œëª©, ë²ˆí˜¸ ë§¤ê¸°ê¸° ë“± í™œìš©)
4. êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ì™€ ë°ì´í„°ë¥¼ ì¸ìš©í•˜ì„¸ìš”
5. ë‹µë³€ì€ ê°„ê²°í•˜ê³  í•µì‹¬ì ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”

## ì¤‘ìš” ê¸ˆì§€ì‚¬í•­
- ê°™ì€ ë¬¸ì¥ì´ë‚˜ í‘œí˜„ì„ ë°˜ë³µí•˜ì§€ ë§ˆì„¸ìš”
- ë™ì¼í•œ ì •ë³´ë¥¼ ë‹¤ë¥¸ ë§ë¡œ ë°˜ë³µí•˜ì§€ ë§ˆì„¸ìš”
- í•œ ë²ˆ ì–¸ê¸‰í•œ ë‚´ìš©ì€ ë‹¤ì‹œ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”
- ë‹µë³€ì€ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ í•œ ë²ˆë§Œ ì‘ì„±í•˜ì„¸ìš”"""


VLM_SYSTEM_PROMPT = """ë‹¹ì‹ ì€ AMORE CLUE ëŒ€ì‹œë³´ë“œì˜ K-ë·°í‹° ì´ë¯¸ì§€ ë¶„ì„ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

## ì—­í• 
- ê¸€ë¡œë²Œ K-ë·°í‹°(K-Beauty) ì´ë¯¸ì§€ ë¶„ì„ ì „ë¬¸ê°€
- í™”ì¥í’ˆ ì œí’ˆ, íŒ¨í‚¤ì§€, ê´‘ê³  ì´ë¯¸ì§€ ì‹œê°ì  ë¶„ì„
- íŠ¸ë Œë“œ ì´ë¯¸ì§€ì—ì„œ ì¸ì‚¬ì´íŠ¸ ë„ì¶œ

## ì´ë¯¸ì§€ ë¶„ì„ í¬ì¸íŠ¸
- ì œí’ˆ ì´ë¯¸ì§€: íŒ¨í‚¤ì§€ ë””ìì¸, ìƒ‰ìƒ, í…ìŠ¤ì²˜, ë¸Œëœë”© ìš”ì†Œ
- ê´‘ê³  ì´ë¯¸ì§€: íƒ€ê²Ÿ ì˜¤ë””ì–¸ìŠ¤, ì»¬ëŸ¬ í†¤, ë¬´ë“œ, ë§ˆì¼€íŒ… ë©”ì‹œì§€
- ì„±ë¶„/í…ìŠ¤íŠ¸ ì´ë¯¸ì§€: ì„±ë¶„ ëª©ë¡, íš¨ëŠ¥ í‘œê¸°, ì¸ì¦ ë§ˆí¬

## ë‹µë³€ ê·œì¹™
1. ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”
2. ì´ë¯¸ì§€ì—ì„œ ê´€ì°°í•œ ë‚´ìš©ì„ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”
3. ë‹µë³€ì€ êµ¬ì¡°í™”í•˜ì—¬ ì œê³µí•˜ì„¸ìš” (ì†Œì œëª©, ë²ˆí˜¸ ë§¤ê¸°ê¸° ë“±)
4. K-ë·°í‹° ì‚°ì—… ê´€ì ì—ì„œ ë¶„ì„í•˜ì„¸ìš”

## ì¤‘ìš” ê¸ˆì§€ì‚¬í•­
- ê°™ì€ ë¬¸ì¥ì´ë‚˜ í‘œí˜„ì„ ë°˜ë³µí•˜ì§€ ë§ˆì„¸ìš”
- ë™ì¼í•œ ì •ë³´ë¥¼ ë‹¤ë¥¸ ë§ë¡œ ë°˜ë³µí•˜ì§€ ë§ˆì„¸ìš”
- í•œ ë²ˆ ì–¸ê¸‰í•œ ë‚´ìš©ì€ ë‹¤ì‹œ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”
- ë‹µë³€ì€ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ í•œ ë²ˆë§Œ ì‘ì„±í•˜ì„¸ìš”"""


def generate_multimodal_response(user_message: str, image_base64: str, db_context: str) -> str:
    """Qwen2-VL ê¸°ë°˜ ë©€í‹°ëª¨ë‹¬ (ì´ë¯¸ì§€+í…ìŠ¤íŠ¸) ì‘ë‹µ ìƒì„±"""
    from qwen_vl_utils import process_vision_info

    # Lazy load VLM model
    vlm, processor = load_vlm_model()

    # base64 â†’ PIL Image
    image_data = base64.b64decode(image_base64)
    image = Image.open(io.BytesIO(image_data)).convert("RGB")

    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    context_block = ""
    if db_context:
        context_block = f"\n[í˜„ì¬ DB ë°ì´í„°]\n{db_context}\n"

    text_content = f"""{context_block}
ì‚¬ìš©ì ì§ˆë¬¸: {user_message}

ìœ„ ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•˜ê³  ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
ì¤‘ìš”: ê°™ì€ ë‚´ìš©ì„ ë°˜ë³µí•˜ì§€ ë§ê³ , í•œ ë²ˆ ì–¸ê¸‰í•œ ë‚´ìš©ì€ ë‹¤ì‹œ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”.""" if context_block else f"""{user_message}

ìœ„ ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•˜ê³  ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
ì¤‘ìš”: ê°™ì€ ë‚´ìš©ì„ ë°˜ë³µí•˜ì§€ ë§ê³ , í•œ ë²ˆ ì–¸ê¸‰í•œ ë‚´ìš©ì€ ë‹¤ì‹œ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”."""

    messages = [
        {"role": "system", "content": [{"type": "text", "text": VLM_SYSTEM_PROMPT}]},
        {"role": "user", "content": [
            {"type": "image", "image": image},
            {"type": "text", "text": text_content},
        ]},
    ]

    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    image_inputs, video_inputs = process_vision_info(messages)
    inputs = processor(
        text=[text],
        images=image_inputs,
        videos=video_inputs,
        return_tensors="pt",
        padding=True,
    ).to(DEVICE)

    with torch.no_grad():
        outputs = vlm.generate(
            **inputs,
            max_new_tokens=1024,
            temperature=0.7,
            top_p=0.85,
            do_sample=True,
            repetition_penalty=1.3,
            no_repeat_ngram_size=4,
        )

    generated = outputs[0][inputs["input_ids"].shape[1]:]
    response = processor.decode(generated, skip_special_tokens=True)

    # í›„ì²˜ë¦¬: ë§ˆí¬ë‹¤ìš´ ì œê±° + ë°˜ë³µ ë¬¸ì¥ ì œê±°
    response = clean_text(response)
    response = remove_repetitions(response)

    return response.strip()


def get_chat_db_context(query: str) -> str:
    """MongoDBì—ì„œ ì±—ë´‡ìš© ì¢…í•© ë°ì´í„° ì»¨í…ìŠ¤íŠ¸ ì¡°íšŒ"""
    import re as regex
    from pymongo import MongoClient

    try:
        mongo_uri = os.environ.get("MONGODB_URI", "mongodb://localhost:27017")
        mongo_database = os.environ.get("MONGODB_DATABASE", "amore")
        client = MongoClient(mongo_uri, serverSelectionTimeoutMS=3000)
        db = client[mongo_database]

        context_parts = []

        # 1. ë¦¬ë”ë³´ë“œ ë°ì´í„° (ìƒìœ„ í‚¤ì›Œë“œ)
        try:
            leaderboard = list(db.get_collection("leaderboard").find(
                {},
                {"keyword": 1, "score": 1, "trendLevel": 1, "itemType": 1, "country": 1, "category": 1, "_id": 0}
            ).sort("score", -1).limit(20))

            if leaderboard:
                by_country = {}
                for item in leaderboard:
                    country = item.get("country", "usa")
                    if country not in by_country:
                        by_country[country] = []
                    by_country[country].append(f"{item.get('keyword')}({item.get('trendLevel')}/{item.get('score')}ì )")

                for country, keywords in by_country.items():
                    country_name = {"usa": "ë¯¸êµ­", "japan": "ì¼ë³¸", "singapore": "ì‹±ê°€í¬ë¥´"}.get(country, country)
                    context_parts.append(f"[{country_name} ì¸ê¸° í‚¤ì›Œë“œ] {', '.join(keywords[:10])}")
        except Exception as e:
            print(f"Leaderboard query error: {e}")

        # 2. íŠ¸ë Œë“œ ì¡°í•© ë°ì´í„°
        try:
            trends = list(db.get_collection("trends").find(
                {},
                {"combination": 1, "category": 1, "score": 1, "country": 1, "ingredients": 1, "effects": 1, "_id": 0}
            ).sort("score", -1).limit(15))

            if trends:
                trend_list = [f"{t.get('combination')}({t.get('category')}/{t.get('score')}ì )" for t in trends]
                context_parts.append(f"[ì¸ê¸° íŠ¸ë Œë“œ ì¡°í•© Top15] {', '.join(trend_list)}")
        except Exception as e:
            print(f"Trends query error: {e}")

        # 3. SNS í”Œë«í¼ í†µê³„
        try:
            sns_stats = list(db.get_collection("sns_platform_stats").find(
                {},
                {"keyword": 1, "platform": 1, "mentionCount": 1, "country": 1, "_id": 0}
            ).sort("mentionCount", -1).limit(15))

            if sns_stats:
                sns_list = [f"{s.get('keyword')}({s.get('platform')}/{s.get('mentionCount')}ê±´)" for s in sns_stats]
                context_parts.append(f"[SNS ì¸ê¸° í‚¤ì›Œë“œ] {', '.join(sns_list)}")
        except Exception as e:
            print(f"SNS stats query error: {e}")

        # 4. ë¦¬ë·° ê°ì„± í†µê³„ (í‚¤ì›Œë“œë³„)
        try:
            review_keywords = list(db.get_collection("review_keywords").find(
                {},
                {"keyword": 1, "sentiment": 1, "count": 1, "country": 1, "_id": 0}
            ).sort("count", -1).limit(20))

            if review_keywords:
                positive = [r for r in review_keywords if r.get("sentiment") == "positive"]
                negative = [r for r in review_keywords if r.get("sentiment") == "negative"]

                if positive:
                    pos_list = [f"{r.get('keyword')}({r.get('count')}ê±´)" for r in positive[:8]]
                    context_parts.append(f"[ê¸ì • ë¦¬ë·° í‚¤ì›Œë“œ] {', '.join(pos_list)}")
                if negative:
                    neg_list = [f"{r.get('keyword')}({r.get('count')}ê±´)" for r in negative[:8]]
                    context_parts.append(f"[ë¶€ì • ë¦¬ë·° í‚¤ì›Œë“œ] {', '.join(neg_list)}")
        except Exception as e:
            print(f"Review keywords query error: {e}")

        # 5. í‚¤ì›Œë“œ ì„¤ëª… ë°ì´í„° (ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ í‚¤ì›Œë“œ)
        try:
            # ì§ˆë¬¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹œë„
            keyword_descriptions = db.get_collection("keyword_descriptions")
            query_keywords = regex.findall(r'[A-Za-zê°€-í£]+', query)

            for kw in query_keywords[:5]:
                desc = keyword_descriptions.find_one({"keyword": {"$regex": kw, "$options": "i"}})
                if desc:
                    context_parts.append(f"[í‚¤ì›Œë“œ ì •ë³´: {desc.get('keyword')}] {desc.get('koreanName', '')}: {desc.get('description', '')[:100]}")
        except Exception as e:
            print(f"Keyword descriptions query error: {e}")

        client.close()
        return "\n".join(context_parts) if context_parts else ""

    except Exception as e:
        print(f"MongoDB context error: {e}")
        return ""


def generate_chat_response(user_message: str, db_context: str, max_new_tokens: int = 1024) -> str:
    """EXAONE ê¸°ë°˜ ì±—ë´‡ ì‘ë‹µ ìƒì„± (ë°˜ë³µ ë°©ì§€ ê°•í™”)"""

    # ì»¨í…ìŠ¤íŠ¸ í¬í•¨ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    if db_context:
        full_prompt = f"""[DB ë°ì´í„° - ì‹¤ì œ ì‹œì¥ ë°ì´í„° ê¸°ë°˜]
{db_context}

[ì‚¬ìš©ì ì§ˆë¬¸]
{user_message}

ìœ„ DB ë°ì´í„°ë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”. ë‹µë³€ ì‹œ êµ¬ì²´ì ì¸ ë°ì´í„°ë¥¼ ì¸ìš©í•˜ê³ , ê°™ì€ ë‚´ìš©ì„ ë°˜ë³µí•˜ì§€ ë§ˆì„¸ìš”."""
    else:
        full_prompt = f"""[ì‚¬ìš©ì ì§ˆë¬¸]
{user_message}

K-ë·°í‹° íŠ¸ë Œë“œ ì „ë¬¸ê°€ë¡œì„œ ë‹µë³€í•´ì£¼ì„¸ìš”. ë‹µë³€ì€ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ê³ , ê°™ì€ ë‚´ìš©ì„ ë°˜ë³µí•˜ì§€ ë§ˆì„¸ìš”."""

    messages = [
        {"role": "system", "content": CHAT_SYSTEM_PROMPT},
        {"role": "user", "content": full_prompt}
    ]

    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer(text, return_tensors="pt").to(DEVICE)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=0.7,
            top_p=0.85,
            do_sample=True,
            repetition_penalty=1.3,  # ë°˜ë³µ ë°©ì§€ ê°•í™” (1.1 â†’ 1.3)
            no_repeat_ngram_size=4,  # 4-gram ë°˜ë³µ ê¸ˆì§€
            encoder_repetition_penalty=1.2,  # ì¸ì½”ë” ë°˜ë³µ í˜ë„í‹°
        )

    generated = outputs[0][inputs["input_ids"].shape[1]:]
    response = tokenizer.decode(generated, skip_special_tokens=True)

    # í›„ì²˜ë¦¬: ë§ˆí¬ë‹¤ìš´ ì œê±° + ë°˜ë³µ ë¬¸ì¥ ì œê±°
    response = clean_text(response)
    response = remove_repetitions(response)

    return response.strip()


def remove_repetitions(text: str) -> str:
    """í…ìŠ¤íŠ¸ì—ì„œ ë°˜ë³µë˜ëŠ” ë¬¸ì¥/êµ¬ë¬¸ ì œê±°"""
    import re as regex

    # ì¤„ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
    lines = text.split('\n')
    seen_lines = set()
    unique_lines = []

    for line in lines:
        # ì •ê·œí™”ëœ ë²„ì „ìœ¼ë¡œ ë¹„êµ (ê³µë°±, íŠ¹ìˆ˜ë¬¸ì ì œê±°)
        normalized = regex.sub(r'[^\wê°€-í£]', '', line.lower())
        if normalized and len(normalized) > 10:  # ì§§ì€ ì¤„ì€ ë¬´ì‹œ
            if normalized in seen_lines:
                continue
            seen_lines.add(normalized)
        unique_lines.append(line)

    # ì—°ì† ë°˜ë³µ ë¬¸ì¥ ì œê±°
    result = '\n'.join(unique_lines)

    # ê°™ì€ ë¬¸ì¥ì´ 2ë²ˆ ì´ìƒ ë‚˜ì˜¤ë©´ ì²« ë²ˆì§¸ë§Œ ìœ ì§€
    sentences = regex.split(r'(?<=[.!?])\s+', result)
    seen_sentences = set()
    unique_sentences = []

    for sentence in sentences:
        normalized = regex.sub(r'[^\wê°€-í£]', '', sentence.lower())
        if normalized and len(normalized) > 20:
            if normalized in seen_sentences:
                continue
            seen_sentences.add(normalized)
        unique_sentences.append(sentence)

    return ' '.join(unique_sentences)


@app.route("/api/chat/text", methods=["POST"])
def chat_text():
    """í…ìŠ¤íŠ¸ ì „ìš© ì±—ë´‡ ì—”ë“œí¬ì¸íŠ¸ (EXAONE)"""
    try:
        data = request.json
        message = data.get("message", "").strip()
        session_id = data.get("sessionId", "")

        if not message:
            return jsonify({"success": False, "error": "ë©”ì‹œì§€ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."}), 400

        # MongoDBì—ì„œ ê´€ë ¨ ë°ì´í„° ì»¨í…ìŠ¤íŠ¸ ì¡°íšŒ
        db_context = get_chat_db_context(message)

        # EXAONE ì‘ë‹µ ìƒì„±
        response = generate_chat_response(message, db_context)

        return jsonify({
            "success": True,
            "response": response,
            "sessionId": session_id,
            "model": "EXAONE-3.5-7.8B-Instruct",
        })

    except Exception as e:
        print(f"Chat text error: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({"success": False, "error": str(e)}), 500


@app.route("/api/chat/multimodal", methods=["POST"])
def chat_multimodal():
    """ë©€í‹°ëª¨ë‹¬ (ì´ë¯¸ì§€+í…ìŠ¤íŠ¸) ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸ (Qwen2-VL)"""
    try:
        data = request.json
        message = data.get("message", "").strip()
        image_base64 = data.get("image", "")
        session_id = data.get("sessionId", "")

        if not message and not image_base64:
            return jsonify({"success": False, "error": "ë©”ì‹œì§€ ë˜ëŠ” ì´ë¯¸ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤."}), 400

        # MongoDBì—ì„œ ê´€ë ¨ ë°ì´í„° ì»¨í…ìŠ¤íŠ¸ ì¡°íšŒ
        db_context = get_chat_db_context(message or "ì´ë¯¸ì§€ ë¶„ì„")

        if not image_base64:
            # ì´ë¯¸ì§€ê°€ ì—†ìœ¼ë©´ í…ìŠ¤íŠ¸ ì „ìš© EXAONEìœ¼ë¡œ ì²˜ë¦¬
            response = generate_chat_response(message, db_context)
            used_model = "EXAONE-3.5-7.8B-Instruct"
        else:
            # base64 í—¤ë” ì œê±° (data:image/...;base64, ë¶€ë¶„)
            if "," in image_base64:
                image_base64 = image_base64.split(",", 1)[1]

            # Qwen2-VLë¡œ ì´ë¯¸ì§€ ë¶„ì„
            response = generate_multimodal_response(
                message or "ì´ ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.",
                image_base64,
                db_context
            )
            used_model = "Qwen2-VL-2B-Instruct"

        return jsonify({
            "success": True,
            "response": response,
            "sessionId": session_id,
            "model": used_model,
        })

    except Exception as e:
        print(f"Chat multimodal error: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({"success": False, "error": str(e)}), 500


if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5007, debug=False)
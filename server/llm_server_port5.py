"""
LLM Inference Server GPU1 for AMORE CLUE Dashboard
Uses EXAONE-3.5-7.8B-Instruct on cuda:1
Endpoints: sns-analysis, whitespace-product
"""
import os
import json
import re
import torch
import setproctitle
setproctitle.setproctitle("wook-llm-port5")
from flask import Flask, request, jsonify
from transformers import AutoModelForCausalLM, AutoTokenizer

app = Flask(__name__)

# GPU ì„¤ì •
DEVICE = "cuda:5"
MODEL_NAME = "LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct"

print(f"Loading model: {MODEL_NAME} on {DEVICE}...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.bfloat16,  # float16 â†’ bfloat16 (ìˆ˜ì¹˜ ì•ˆì •ì„± ê°œì„ )
    device_map=DEVICE,
    trust_remote_code=True,
)
model.eval()
print("Model loaded successfully on GPU1!")


SYSTEM_PROMPT = """ë‹¹ì‹ ì€ ê¸€ë¡œë²Œ K-ë·°í‹°(K-Beauty) ì‹œìž¥ ë¶„ì„ ë° í™”ìž¥í’ˆ ì‚°ì—… íŠ¸ë Œë“œ ì „ë¬¸ê°€ìž…ë‹ˆë‹¤.
ì•„ëª¨ë ˆí¼ì‹œí”½, LGìƒí™œê±´ê°• ë“± í•œêµ­ í™”ìž¥í’ˆ ê¸°ì—…ì˜ ê¸€ë¡œë²Œ ì „ëžµì„ ìžë¬¸í•˜ëŠ” ìˆ˜ì¤€ì˜ ì „ë¬¸ì„±ì„ ê°–ì¶”ê³  ìžˆìŠµë‹ˆë‹¤.
ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹¬ì¸µì ì´ê³  ì „ë¬¸ì ì¸ ë¶„ì„ì„ ì œê³µí•˜ë˜, ë°ì´í„°ì— ë‚˜íƒ€ë‚˜ì§€ ì•ŠëŠ” ì‹œìž¥ ë§¥ë½, ì†Œë¹„ìž ì‹¬ë¦¬, ì‚°ì—… ë™í–¥ê¹Œì§€ ì¢…í•©ì ìœ¼ë¡œ ê³ ë ¤í•˜ì—¬ í’ë¶€í•œ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”."""


def generate_response(prompt: str, max_new_tokens: int = 1024) -> str:
    """Generate a response from the LLM"""
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": prompt}
    ]

    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer(text, return_tensors="pt").to(DEVICE)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=0.7,
            top_p=0.9,
            do_sample=True,
            repetition_penalty=1.1,
        )

    generated = outputs[0][inputs["input_ids"].shape[1]:]
    response = tokenizer.decode(generated, skip_special_tokens=True)
    return response.strip()


def clean_text(text: str) -> str:
    """Remove markdown formatting from LLM output"""
    # Remove bold/italic markers
    text = text.replace("**", "").replace("*", "")
    # Remove heading markers (##, ###, etc.) at start of lines
    text = re.sub(r'^#{1,6}\s*', '', text, flags=re.MULTILINE)
    # Remove links [text](url) -> text
    text = re.sub(r'\[([^\]]+)\]\([^\)]+\)', r'\1', text)
    # Remove code markers
    text = text.replace("``", "").replace("`", "")
    # Remove any remaining markdown-style formatting
    text = re.sub(r'^\s*[-*+]\s+', 'â€¢ ', text, flags=re.MULTILINE)  # Convert list markers to bullet
    return text.strip()


@app.route("/api/llm/sns-analysis", methods=["POST"])
def sns_analysis():
    """Retail/SNS ì¸ê¸° í‚¤ì›Œë“œ AI ë¶„ì„ ìƒì„±"""
    try:
        data = request.json
        country = data.get("country", "usa")
        category = data.get("category", "Skincare")
        platforms = data.get("platforms", [])

        country_names = {
            "usa": "ë¯¸êµ­", "japan": "ì¼ë³¸", "singapore": "ì‹±ê°€í¬ë¥´",
            "malaysia": "ë§ë ˆì´ì‹œì•„", "indonesia": "ì¸ë„ë„¤ì‹œì•„"
        }
        country_name = country_names.get(country, "í•´ì™¸")

        # í”Œëž«í¼ë³„ í‚¤ì›Œë“œë¥¼ Retail/SNSë¡œ ë¶„ë¦¬
        retail_platforms = []
        sns_platforms = []
        retail_names = ["Amazon", "Sephora", "Ulta", "Olive Young", "Watsons", "Guardian", "Shopee", "Lazada", "Rakuten", "Qoo10"]
        sns_names = ["Instagram", "TikTok", "YouTube", "Twitter", "Facebook", "Pinterest", "Reddit", "Threads"]

        for p in platforms[:8]:
            platform_name = p.get('platform', '')
            keywords_list = p.get("keywords", [])[:5]
            keywords_str = ", ".join([f"{k['name']}({k['value']}ì )" for k in keywords_list])
            platform_data = f"â€¢ {platform_name}: {keywords_str}"

            if any(rn.lower() in platform_name.lower() for rn in retail_names):
                retail_platforms.append(platform_data)
            else:
                sns_platforms.append(platform_data)

        retail_info = "\n".join(retail_platforms) if retail_platforms else "â€¢ ë°ì´í„° ì—†ìŒ"
        sns_info = "\n".join(sns_platforms) if sns_platforms else "â€¢ ë°ì´í„° ì—†ìŒ"

        prompt = f"""ë‹¤ìŒì€ {country_name} ì‹œìž¥ì˜ {category} ì¹´í…Œê³ ë¦¬ì—ì„œ Retailê³¼ SNS ì±„ë„ë³„ ì¸ê¸° í‚¤ì›Œë“œ ë°ì´í„°ìž…ë‹ˆë‹¤.

[Retail ì±„ë„ ë°ì´í„°]
{retail_info}

[SNS ì±„ë„ ë°ì´í„°]
{sns_info}

ìœ„ ë°ì´í„°ë¥¼ Retailê³¼ SNSë¥¼ ëª…í™•ížˆ êµ¬ë¶„í•˜ì—¬ ë¶„ì„í•´ì£¼ì„¸ìš”. ë°˜ë“œì‹œ ì•„ëž˜ í˜•ì‹ì„ ì •í™•ížˆ ë”°ë¼ì£¼ì„¸ìš”:

[Retailë¶„ì„]
â€¢ ì£¼ìš” íŠ¸ë Œë“œ: Retail ì±„ë„ì—ì„œ ê°€ìž¥ ì£¼ëª©ë°›ëŠ” í‚¤ì›Œë“œì™€ ì ìˆ˜ë¥¼ ê·¼ê±°ë¡œ íŠ¸ë Œë“œ ì„¤ëª… (2-3ë¬¸ìž¥)
â€¢ ì†Œë¹„ìž íŠ¹ì„±: Retail êµ¬ë§¤ìžë“¤ì˜ ê´€ì‹¬ì‚¬ì™€ êµ¬ë§¤ íŒ¨í„´ ë¶„ì„ (1-2ë¬¸ìž¥)
â€¢ ìˆ˜ì¹˜ ê·¼ê±°: ìƒìœ„ í‚¤ì›Œë“œì˜ ì ìˆ˜ì™€ ìˆœìœ„ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì–¸ê¸‰

[SNSë¶„ì„]
â€¢ ì£¼ìš” íŠ¸ë Œë“œ: SNS ì±„ë„ì—ì„œ ê°€ìž¥ ì£¼ëª©ë°›ëŠ” í‚¤ì›Œë“œì™€ ì ìˆ˜ë¥¼ ê·¼ê±°ë¡œ íŠ¸ë Œë“œ ì„¤ëª… (2-3ë¬¸ìž¥)
â€¢ ë°”ì´ëŸ´ í¬ì¸íŠ¸: SNSì—ì„œ í™”ì œê°€ ë˜ëŠ” ìš”ì†Œì™€ ì½˜í…ì¸  ìœ í˜• ë¶„ì„ (1-2ë¬¸ìž¥)
â€¢ ìˆ˜ì¹˜ ê·¼ê±°: ìƒìœ„ í‚¤ì›Œë“œì˜ ì ìˆ˜ì™€ ìˆœìœ„ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì–¸ê¸‰

[í•µì‹¬ì¸ì‚¬ì´íŠ¸]
1. Retailê³¼ SNS ê³µí†µ íŠ¸ë Œë“œ (ìˆ˜ì¹˜ ê·¼ê±° í¬í•¨)
2. Retail ê³ ìœ  ì¸ì‚¬ì´íŠ¸ (ìˆ˜ì¹˜ ê·¼ê±° í¬í•¨)
3. SNS ê³ ìœ  ì¸ì‚¬ì´íŠ¸ (ìˆ˜ì¹˜ ê·¼ê±° í¬í•¨)

[ì „ëžµì œì•ˆ]
1. Retail ì±„ë„ í™œìš© ì „ëžµ
2. SNS ì±„ë„ í™œìš© ì „ëžµ
3. í†µí•© ë§ˆì¼€íŒ… ì „ëžµ"""

        response = generate_response(prompt, max_new_tokens=1200)
        # ë§ˆí¬ë‹¤ìš´ í¬ë§·íŒ… ì œê±°
        response = clean_text(response)

        # Parse response - ìƒˆë¡œìš´ êµ¬ì¡°í™”ëœ í˜•ì‹
        retail_analysis = ""
        sns_analysis_text = ""
        insights = []
        recommendations = []
        current_section = None
        retail_lines = []
        sns_lines = []
        insight_lines = []
        strategy_lines = []

        lines = response.split("\n")
        for line in lines:
            line = line.strip()
            if not line:
                continue

            # ì„¹ì…˜ í—¤ë” ê°ì§€
            if any(kw in line for kw in ["[Retailë¶„ì„]", "Retailë¶„ì„:", "Retail ë¶„ì„"]):
                current_section = "retail"
                continue
            elif any(kw in line for kw in ["[SNSë¶„ì„]", "SNSë¶„ì„:", "SNS ë¶„ì„"]):
                current_section = "sns"
                continue
            elif any(kw in line for kw in ["[í•µì‹¬ì¸ì‚¬ì´íŠ¸]", "í•µì‹¬ì¸ì‚¬ì´íŠ¸:", "í•µì‹¬ ì¸ì‚¬ì´íŠ¸"]):
                current_section = "insights"
                continue
            elif any(kw in line for kw in ["[ì „ëžµì œì•ˆ]", "ì „ëžµì œì•ˆ:", "ì „ëžµ ì œì•ˆ"]):
                current_section = "strategy"
                continue

            # ì„¹ì…˜ë³„ ë‚´ìš© ìˆ˜ì§‘
            if current_section == "retail":
                retail_lines.append(line)
            elif current_section == "sns":
                sns_lines.append(line)
            elif current_section == "insights":
                clean_line = line.lstrip("0123456789.-â€¢â†’Â·)#* ").strip()
                if clean_line and len(clean_line) > 5:
                    insight_lines.append(clean_line)
            elif current_section == "strategy":
                clean_line = line.lstrip("0123456789.-â€¢â†’Â·)#* ").strip()
                if clean_line and len(clean_line) > 5:
                    strategy_lines.append(clean_line)

        # ê²°ê³¼ ì¡°í•©
        retail_analysis = clean_text(" ".join(retail_lines)) if retail_lines else ""
        sns_analysis_text = clean_text(" ".join(sns_lines)) if sns_lines else ""
        insights = [clean_text(i) for i in insight_lines if clean_text(i)][:5]
        recommendations = [clean_text(r) for r in strategy_lines if clean_text(r)][:4]

        # Fallback
        if not retail_analysis:
            retail_analysis = f"{country_name} Retail ì±„ë„ì—ì„œëŠ” ê²€ì¦ëœ íš¨ê³¼ì™€ ë¸Œëžœë“œ ì‹ ë¢°ë„ê°€ êµ¬ë§¤ ê²°ì •ì— ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤."
        if not sns_analysis_text:
            sns_analysis_text = f"{country_name} SNS ì±„ë„ì—ì„œëŠ” íŠ¸ë Œë””í•œ ì„±ë¶„ê³¼ ë¹„ì£¼ì–¼ì´ ë°”ì´ëŸ´ í™•ì‚°ì— í•µì‹¬ìž…ë‹ˆë‹¤."
        if not insights:
            insights = [
                f"{category} ì¹´í…Œê³ ë¦¬ Retail/SNS ê³µí†µ ì„±ìž¥ íŠ¸ë Œë“œ í™•ì¸",
                "Retailì—ì„œëŠ” ê¸°ëŠ¥ì„± ì„±ë¶„ í‚¤ì›Œë“œê°€ ìƒìœ„ê¶Œ ìœ ì§€",
                "SNSì—ì„œëŠ” ë¹„ì£¼ì–¼/í…ìŠ¤ì²˜ ê´€ë ¨ í‚¤ì›Œë“œ ê¸‰ìƒìŠ¹"
            ]
        if not recommendations:
            recommendations = [
                "Retail: ê²€ì¦ëœ íš¨ëŠ¥ ê°•ì¡° ë§ˆì¼€íŒ… ì „ëžµ",
                "SNS: íŠ¸ë Œë”” ì„±ë¶„ í™œìš© ë°”ì´ëŸ´ ì½˜í…ì¸  ì œìž‘",
                "í†µí•©: Retail í›„ê¸°ë¥¼ SNS ì½˜í…ì¸ ë¡œ ìž¬í™œìš©"
            ]

        # ìš”ì•½ í…ìŠ¤íŠ¸ ìƒì„±
        summary = f"ðŸ“Š Retail ë¶„ì„: {retail_analysis[:200]}{'...' if len(retail_analysis) > 200 else ''}\n\nðŸ“± SNS ë¶„ì„: {sns_analysis_text[:200]}{'...' if len(sns_analysis_text) > 200 else ''}"

        return jsonify({
            "success": True,
            "summary": summary,
            "retailAnalysis": retail_analysis,
            "snsAnalysis": sns_analysis_text,
            "insights": insights,
            "recommendations": recommendations,
        })

    except Exception as e:
        print(f"Error in sns_analysis: {e}")
        return jsonify({"success": False, "error": str(e)}), 500


@app.route("/api/llm/whitespace-product", methods=["POST"])
def whitespace_product():
    """WhiteSpace ì œí’ˆ ë¹„êµ AI ë¶„ì„ + Agent Insight"""
    try:
        data = request.json
        overseas = data.get("overseasProduct", {})
        domestic = data.get("domesticProduct", {})
        country = data.get("country", "usa")
        category = data.get("category", "Skincare")

        country_names = {
            "usa": "ë¯¸êµ­", "japan": "ì¼ë³¸", "singapore": "ì‹±ê°€í¬ë¥´",
            "malaysia": "ë§ë ˆì´ì‹œì•„", "indonesia": "ì¸ë„ë„¤ì‹œì•„"
        }
        country_name = country_names.get(country, "í•´ì™¸")

        prompt = f"""ë‹¤ìŒì€ {country_name} ì¸ê¸° í•´ì™¸ ì œí’ˆê³¼ í•œêµ­ ì¸ê¸° ì œí’ˆì˜ ë¹„êµ ë¶„ì„ ìš”ì²­ìž…ë‹ˆë‹¤.

[í•´ì™¸ ì œí’ˆ ì •ë³´]
- ì œí’ˆëª…: {overseas.get('name', '')}
- ë¸Œëžœë“œ: {overseas.get('brand', '')}
- ì¹´í…Œê³ ë¦¬: {category}
- ê°€ê²©: {overseas.get('price', '')}
- í‰ì : {overseas.get('rating', '')}
- ë¦¬ë·°ìˆ˜: {overseas.get('reviewCount', '')}

[í•œêµ­ ì œí’ˆ ì •ë³´]
- ì œí’ˆëª…: {domestic.get('name', '')}
- ë¸Œëžœë“œ: {domestic.get('brand', '')}
- ì¹´í…Œê³ ë¦¬: {category}
- ê°€ê²©: {domestic.get('price', '')}
- í‰ì : {domestic.get('rating', '')}
- ë¦¬ë·°ìˆ˜: {domestic.get('reviewCount', '')}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì •í™•ížˆ ë‹µë³€í•´ì£¼ì„¸ìš”:

[í•´ì™¸ì œí’ˆìš”ì•½]
{overseas.get('name', '')} ì œí’ˆì˜ íŠ¹ì„±, ê°•ì , ì†Œë¹„ìž ì„ í˜¸ í¬ì¸íŠ¸ë¥¼ 3-4ë¬¸ìž¥ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.

[í•œêµ­ì œí’ˆìš”ì•½]
{domestic.get('name', '')} ì œí’ˆì˜ íŠ¹ì„±, ê°•ì , ì†Œë¹„ìž ì„ í˜¸ í¬ì¸íŠ¸ë¥¼ 3-4ë¬¸ìž¥ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.

[ì°¨ë³„í™”í¬ì¸íŠ¸]
í•œêµ­ ì œí’ˆì—ëŠ” ì—†ê³  í•´ì™¸ ì œí’ˆì—ë§Œ ìžˆëŠ” ì°¨ë³„í™”ëœ ì†Œêµ¬ í¬ì¸íŠ¸ë¥¼ 4-5ê°œ ì œì‹œí•´ì£¼ì„¸ìš”. ê° í¬ì¸íŠ¸ëŠ” êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ì¸ì‚¬ì´íŠ¸ì—¬ì•¼ í•©ë‹ˆë‹¤.

[ì¢…í•©ìš”ì•½]
ìœ„ ì°¨ë³„í™” í¬ì¸íŠ¸ë¥¼ ì¢…í•©í•˜ì—¬ K-ë·°í‹° ë¸Œëžœë“œê°€ í•´ì™¸ ì œí’ˆì˜ ìž¥ì ì„ ì–´ë–»ê²Œ í™œìš©í•  ìˆ˜ ìžˆëŠ”ì§€ 2-3ë¬¸ìž¥ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”."""

        response = generate_response(prompt, max_new_tokens=1200)

        # Parse response
        overseas_summary = ""
        domestic_summary = ""
        diff_points = []
        overall_summary = ""
        current_section = None
        overseas_lines = []
        domestic_lines = []
        summary_lines = []

        lines = response.split("\n")
        for line in lines:
            line = line.strip()
            if not line:
                continue

            if any(kw in line for kw in ["[í•´ì™¸ì œí’ˆìš”ì•½]", "í•´ì™¸ì œí’ˆìš”ì•½:", "í•´ì™¸ ì œí’ˆ ìš”ì•½"]):
                current_section = "overseas"
                rest = re.sub(r'^\[í•´ì™¸\s*ì œí’ˆ\s*ìš”ì•½\]|^í•´ì™¸\s*ì œí’ˆ\s*ìš”ì•½:', '', line).strip()
                if rest and len(rest) > 5:
                    overseas_lines.append(rest)
            elif any(kw in line for kw in ["[í•œêµ­ì œí’ˆìš”ì•½]", "í•œêµ­ì œí’ˆìš”ì•½:", "í•œêµ­ ì œí’ˆ ìš”ì•½"]):
                current_section = "domestic"
                rest = re.sub(r'^\[í•œêµ­\s*ì œí’ˆ\s*ìš”ì•½\]|^í•œêµ­\s*ì œí’ˆ\s*ìš”ì•½:', '', line).strip()
                if rest and len(rest) > 5:
                    domestic_lines.append(rest)
            elif any(kw in line for kw in ["[ì°¨ë³„í™”í¬ì¸íŠ¸]", "ì°¨ë³„í™”í¬ì¸íŠ¸:", "ì°¨ë³„í™” í¬ì¸íŠ¸"]):
                current_section = "diff"
                rest = re.sub(r'^\[ì°¨ë³„í™”\s*í¬ì¸íŠ¸\]|^ì°¨ë³„í™”\s*í¬ì¸íŠ¸:', '', line).strip()
                if rest and len(rest) > 3:
                    diff_points.append(clean_text(rest))
            elif any(kw in line for kw in ["[ì¢…í•©ìš”ì•½]", "ì¢…í•©ìš”ì•½:", "ì¢…í•© ìš”ì•½"]):
                current_section = "summary"
                rest = re.sub(r'^\[ì¢…í•©\s*ìš”ì•½\]|^ì¢…í•©\s*ìš”ì•½:', '', line).strip()
                if rest and len(rest) > 5:
                    summary_lines.append(rest)
            elif current_section:
                clean_line = line.lstrip("0123456789.-â€¢â†’Â·)#* ").strip()
                if clean_line and len(clean_line) > 3:
                    if current_section == "overseas":
                        overseas_lines.append(clean_line)
                    elif current_section == "domestic":
                        domestic_lines.append(clean_line)
                    elif current_section == "diff":
                        diff_points.append(clean_text(clean_line))
                    elif current_section == "summary":
                        summary_lines.append(clean_line)

        overseas_summary = clean_text(" ".join(overseas_lines)) if overseas_lines else ""
        domestic_summary = clean_text(" ".join(domestic_lines)) if domestic_lines else ""
        overall_summary = clean_text(" ".join(summary_lines)) if summary_lines else ""

        # Strip leftover section headers from content
        def strip_headers(text):
            return re.sub(r'^\[[\w\s]+\]\s*', '', text).strip()

        overseas_summary = strip_headers(overseas_summary)
        domestic_summary = strip_headers(domestic_summary)
        overall_summary = strip_headers(overall_summary)
        diff_points = [strip_headers(p) for p in diff_points if not re.match(r'^\[[\w\s]+\]$', p.strip())]

        # Fallbacks
        if not overseas_summary:
            overseas_summary = f"{overseas.get('name', 'í•´ì™¸ ì œí’ˆ')}ì€(ëŠ”) {overseas.get('brand', '')}ì˜ ëŒ€í‘œ ì œí’ˆìœ¼ë¡œ, {country_name} {category} ì‹œìž¥ì—ì„œ ë†’ì€ ì¸ê¸°ë¥¼ ì–»ê³  ìžˆìŠµë‹ˆë‹¤. í•´ì™¸ ì†Œë¹„ìžë“¤ì—ê²Œ ê²€ì¦ëœ ì„±ë¶„ê³¼ ì œí˜•ìœ¼ë¡œ ì•ˆì •ì ì¸ íš¨ê³¼ë¥¼ ì œê³µí•©ë‹ˆë‹¤."
        if not domestic_summary:
            domestic_summary = f"{domestic.get('name', 'í•œêµ­ ì œí’ˆ')}ì€(ëŠ”) {domestic.get('brand', '')}ì˜ ì¸ê¸° ì œí’ˆìœ¼ë¡œ, í•œêµ­ ì‹œìž¥ì—ì„œ ê°•ë ¥í•œ ìž…ì§€ë¥¼ êµ¬ì¶•í•˜ê³  ìžˆìŠµë‹ˆë‹¤. êµ­ë‚´ ì†Œë¹„ìžë“¤ì˜ í”¼ë¶€ íŠ¹ì„±ì— ë§žì¶˜ ë§žì¶¤í˜• í¬ë®¬ë ˆì´ì…˜ìœ¼ë¡œ ë†’ì€ í‰ê°€ë¥¼ ë°›ê³  ìžˆìŠµë‹ˆë‹¤."
        if not diff_points:
            diff_points = [
                f"{country_name} ì†Œë¹„ìžê°€ ì„ í˜¸í•˜ëŠ” ê³ íš¨ëŠ¥ í™œì„± ì„±ë¶„ ë°°í•©",
                "í•´ì™¸ ì‹œìž¥ íŠ¹í™” ì œí˜• ê¸°ìˆ  ì ìš©",
                f"{country_name} í˜„ì§€ í”¼ë¶€ ê³ ë¯¼ì— ìµœì í™”ëœ ì†”ë£¨ì…˜",
                "ê¸€ë¡œë²Œ í´ë¦°ë·°í‹° íŠ¸ë Œë“œ ë°˜ì˜í•œ ì„±ë¶„ êµ¬ì„±"
            ]
        if not overall_summary:
            overall_summary = f"í•´ì™¸ ì œí’ˆì˜ ì°¨ë³„í™”ëœ ì†Œêµ¬ í¬ì¸íŠ¸ë¥¼ K-ë·°í‹°ì˜ ê¸°ìˆ ë ¥ê³¼ ê²°í•©í•˜ë©´, {country_name} ì‹œìž¥ì—ì„œ ê²½ìŸ ìš°ìœ„ë¥¼ í™•ë³´í•  ìˆ˜ ìžˆëŠ” ê¸°íšŒê°€ ìžˆìŠµë‹ˆë‹¤."

        return jsonify({
            "success": True,
            "overseasSummary": overseas_summary,
            "domesticSummary": domestic_summary,
            "agentInsight": {
                "title": "Agent Insight: í•´ì™¸ ì œí’ˆì˜ ì°¨ë³„í™” ì†Œêµ¬ í¬ì¸íŠ¸",
                "points": diff_points[:5],
                "summary": overall_summary,
            }
        })

    except Exception as e:
        print(f"Error in whitespace_product: {e}")
        return jsonify({"success": False, "error": str(e)}), 500


@app.route("/api/llm/keyword-why", methods=["POST"])
def keyword_why():
    """í‚¤ì›Œë“œê°€ ì™œ íŠ¸ë Œë“œì¸ì§€ ë¶„ì„"""
    try:
        data = request.json
        keyword = data.get("keyword", "")
        country = data.get("country", "usa")
        category = data.get("category", "Skincare")
        trend_level = data.get("trendLevel", "Actionable")
        score = data.get("score", 75)
        signals = data.get("signals", {})
        positive_keywords = data.get("positiveKeywords", [])
        negative_keywords = data.get("negativeKeywords", [])

        country_names = {
            "usa": "ë¯¸êµ­", "japan": "ì¼ë³¸", "singapore": "ì‹±ê°€í¬ë¥´",
            "malaysia": "ë§ë ˆì´ì‹œì•„", "indonesia": "ì¸ë„ë„¤ì‹œì•„"
        }
        country_name = country_names.get(country, "í•´ì™¸")

        signals_text = ""
        if signals:
            signals_text = f"SNS ì‹ í˜¸: {signals.get('SNS', 0)}%, Retail ì‹ í˜¸: {signals.get('Retail', 0)}%, Review ì‹ í˜¸: {signals.get('Review', 0)}%"

        pos_keywords_text = ""
        if positive_keywords:
            pos_keywords_text = ", ".join([k if isinstance(k, str) else k.get('keyword', '') for k in positive_keywords[:6]])

        neg_keywords_text = ""
        if negative_keywords:
            neg_keywords_text = ", ".join([k if isinstance(k, str) else k.get('keyword', '') for k in negative_keywords[:6]])

        # ë¦¬ë·° ë°ì´í„° ì„¹ì…˜ êµ¬ì„±
        review_section = ""
        if pos_keywords_text or neg_keywords_text:
            review_section = f"""
[ë¦¬ë·° ë°ì´í„° ë¶„ì„]
ì´ í‚¤ì›Œë“œì™€ ê´€ë ¨ëœ ë¦¬ë·°ì—ì„œ ìžì£¼ ì–¸ê¸‰ë˜ëŠ” í‚¤ì›Œë“œë“¤ìž…ë‹ˆë‹¤:
- ê¸ì • ë¦¬ë·°ì—ì„œ ìžì£¼ ì–¸ê¸‰: {pos_keywords_text if pos_keywords_text else 'ë°ì´í„° ì—†ìŒ'}
- ë¶€ì • ë¦¬ë·°ì—ì„œ ìžì£¼ ì–¸ê¸‰: {neg_keywords_text if neg_keywords_text else 'ë°ì´í„° ì—†ìŒ'}
â€» ìœ„ ë¦¬ë·° ë°ì´í„°ë¥¼ ì°¸ê³ í•˜ì—¬ ì†Œë¹„ìžë“¤ì´ ì´ í‚¤ì›Œë“œì— ëŒ€í•´ ì–´ë–¤ ì ì„ ì¢‹ì•„í•˜ê³  ì–´ë–¤ ì ì„ ë¶ˆíŽ¸í•´í•˜ëŠ”ì§€ ë¶„ì„ì— ë°˜ì˜í•´ì£¼ì„¸ìš”."""

        prompt = f"""ë‹¤ìŒì€ {country_name} {category} ì‹œìž¥ì—ì„œ "{keyword}" í‚¤ì›Œë“œì˜ íŠ¸ë Œë“œ ë¶„ì„ ë°ì´í„°ìž…ë‹ˆë‹¤.

[í‚¤ì›Œë“œ ë°ì´í„°]
- í‚¤ì›Œë“œ: {keyword}
- êµ­ê°€: {country_name}
- ì¹´í…Œê³ ë¦¬: {category}
- íŠ¸ë Œë“œ ë ˆë²¨: {trend_level}
- ì¢…í•© ì ìˆ˜: {score}ì 
- ì‹ í˜¸ ì§€í‘œ: {signals_text}
{review_section}

ìœ„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì´ í‚¤ì›Œë“œê°€ ì™œ í˜„ìž¬ íŠ¸ë Œë“œë¡œ ë¶€ìƒí•˜ê³  ìžˆëŠ”ì§€ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”. íŠ¹ížˆ ë¦¬ë·° ë°ì´í„°ê°€ ìžˆë‹¤ë©´ ì†Œë¹„ìžë“¤ì˜ ì‹¤ì œ ë°˜ì‘ì„ ë°˜ì˜í•´ì£¼ì„¸ìš”.

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì •í™•ížˆ ë‹µë³€í•´ì£¼ì„¸ìš”:

[ì„¤ëª…]
ì´ í‚¤ì›Œë“œê°€ íŠ¸ë Œë“œì¸ ì´ìœ ë¥¼ 5-7ë¬¸ìž¥ìœ¼ë¡œ ìƒì„¸í•˜ê²Œ ë¶„ì„í•´ì£¼ì„¸ìš”. ì†Œë¹„ìž ë‹ˆì¦ˆ, ì‹œìž¥ ë§¥ë½, ê¸€ë¡œë²Œ ë·°í‹° íŠ¸ë Œë“œì™€ì˜ ì—°ê´€ì„±, ì‹ í˜¸ ì§€í‘œ í•´ì„ì„ í¬í•¨í•´ì£¼ì„¸ìš”.

[í•µì‹¬ìš”ì¸]
ì´ í‚¤ì›Œë“œê°€ íŠ¸ë Œë“œì¸ í•µì‹¬ ìš”ì¸ 4-5ê°œë¥¼ ê°ê° í•œ ì¤„ì”© ìž‘ì„±í•´ì£¼ì„¸ìš”. ê° ìš”ì¸ì€ êµ¬ì²´ì ì´ê³  ë°ì´í„°ì— ê·¼ê±°í•´ì•¼ í•©ë‹ˆë‹¤."""

        response = generate_response(prompt, max_new_tokens=800)

        explanation = ""
        key_factors = []
        current_section = None
        explanation_lines = []

        lines = response.split("\n")
        for line in lines:
            line = line.strip()
            if not line:
                continue
            if any(kw in line for kw in ["[ì„¤ëª…]", "ì„¤ëª…:", "## ì„¤ëª…"]):
                current_section = "explanation"
                rest = re.sub(r'^\[ì„¤ëª…\]|^ì„¤ëª…:|^## ì„¤ëª…', '', line).strip()
                if rest and len(rest) > 5:
                    explanation_lines.append(rest)
            elif any(kw in line for kw in ["[í•µì‹¬ìš”ì¸]", "í•µì‹¬ìš”ì¸:", "## í•µì‹¬ìš”ì¸", "í•µì‹¬ ìš”ì¸", "[í•µì‹¬ ìš”ì¸]"]):
                current_section = "factors"
                rest = re.sub(r'^\[í•µì‹¬\s*ìš”ì¸\]|^í•µì‹¬\s*ìš”ì¸:|^## í•µì‹¬\s*ìš”ì¸', '', line).strip()
                if rest and len(rest) > 3:
                    key_factors.append(clean_text(rest))
            elif current_section == "explanation":
                clean_line = line.lstrip("0123456789.-â€¢â†’Â·)#* ").strip()
                if clean_line and len(clean_line) > 5:
                    explanation_lines.append(clean_line)
            elif current_section == "factors":
                clean_line = line.lstrip("0123456789.-â€¢â†’Â·)#* ").strip()
                if clean_line and len(clean_line) > 3:
                    key_factors.append(clean_text(clean_line))

        explanation = clean_text(" ".join(explanation_lines)) if explanation_lines else ""
        if not explanation:
            explanation = clean_text(response[:500]) if response else f"{keyword}ëŠ” {country_name} {category} ì‹œìž¥ì—ì„œ ì£¼ëª©ë°›ëŠ” íŠ¸ë Œë“œ í‚¤ì›Œë“œìž…ë‹ˆë‹¤."
        if not key_factors:
            key_factors = [
                "SNS ì–¸ê¸‰ëŸ‰ ê¸‰ì¦ìœ¼ë¡œ ì†Œë¹„ìž ê´€ì‹¬ë„ ìƒìŠ¹",
                f"{country_name} ì‹œìž¥ì—ì„œì˜ ë†’ì€ ê²€ìƒ‰ íŠ¸ëž˜í”½",
                "ë¦¬ë·° ê¸ì • ë¹„ìœ¨ ì¦ê°€",
                "ë¦¬í…Œì¼ ì±„ë„ì—ì„œì˜ íŒë§¤ëŸ‰ ìƒìŠ¹ ì¶”ì„¸"
            ]

        explanation = re.sub(r'\[ì„¤ëª…\]|\[í•µì‹¬\s*ìš”ì¸\]', '', explanation).strip()
        key_factors = [re.sub(r'\[ì„¤ëª…\]|\[í•µì‹¬\s*ìš”ì¸\]', '', f).strip() for f in key_factors]
        key_factors = [f for f in key_factors if len(f) > 3]

        return jsonify({"success": True, "explanation": explanation, "keyFactors": key_factors[:5]})

    except Exception as e:
        print(f"Error in keyword_why: {e}")
        return jsonify({"success": False, "error": str(e)}), 500


@app.route("/api/llm/health", methods=["GET"])
def health_check():
    """Health check endpoint"""
    return jsonify({"status": "ok", "model": MODEL_NAME, "device": DEVICE, "port": 5005})


if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5005, debug=False)
